\contentsline {section}{\numberline {1}Latent Dirichlet Allocation}{2}
\contentsline {subsection}{\numberline {1.1}basics}{2}
\contentsline {subsection}{\numberline {1.2}Gibbs sampler}{4}
\contentsline {section}{\numberline {2}Finding the mixture coefficients for a new 'document' }{4}
\contentsline {subsection}{\numberline {2.1}Just one word}{5}
\contentsline {subsection}{\numberline {2.2}Just two topics}{6}
\contentsline {subsection}{\numberline {2.3}More than two topics}{7}
\contentsline {section}{\numberline {3}LDA for detecting 'blobby' regions of one topic in images }{8}
\contentsline {subsection}{\numberline {3.1}what criterion should we use?}{8}
\contentsline {subsubsection}{\numberline {3.1.1}first: clarify connection between likelihood and KL in general}{8}
\contentsline {subsubsection}{\numberline {3.1.2}what about the 'reverse' KL?...}{9}
\contentsline {section}{\numberline {4}Something that seems to work}{9}
\contentsline {section}{\numberline {5}Bayesian model comparison for locating sources}{12}
\contentsline {subsection}{\numberline {5.1}approximating the score leads to a difference of entropies}{14}
\contentsline {subsection}{\numberline {5.2}eg}{15}
\contentsline {subsection}{\numberline {5.3}softening the boundaries}{15}
\contentsline {section}{\numberline {6}discretization strategies (a.k.a. setting the bin boundaries)}{18}
\contentsline {subsection}{\numberline {6.1}Equal widths}{18}
\contentsline {subsection}{\numberline {6.2}Exponentially increasing widths}{18}
\contentsline {subsection}{\numberline {6.3}Equal occupancies}{18}
\contentsline {subsection}{\numberline {6.4}Exponentially decreasing occupancies}{18}
\contentsline {section}{\numberline {7}`soft' binning?}{19}
\contentsline {subsection}{\numberline {7.1}How to choose the stretch factor?}{21}
\contentsline {subsection}{\numberline {7.2}Anna's idea}{21}
