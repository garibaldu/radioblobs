\documentclass[11pt]{article}
\input{MarcusStyle.sty}  
% I keep MarcusStyle.sty in $HOME/texmf/tex/latex, because 
% kpsewhich -var-value=TEXMFHOME says $HOME/texmf is where latex looks.

\title{topic models, LDA, and all that}
\author{marcus}
\begin{document}
\maketitle


\tableofcontents

\section{Latent Dirichlet Allocation}

\subsection{basics}

\begin{itemize}
\item Set of documents, indexed $d_1, d_2, \ldots$.
\item A fixed vocabulary of words, indexed by $w$.
\item An assumed number of topics, $T$. Topics are indexed by
  $t\in[1,T]$. We will use $t$ to refer to a specific index (eg. $t=3$), but
  $\bt$ means a vector over all values: $t=1..T$.
\item choices for hyperparameters $\alpha_t$ (one for each topic), and $\beta_w$ (one for each word)
\end{itemize}

LDA (latent Dirichlet allocation) treats each document as a bag of
words, so the ``input'' to the whole shebang is set of counts of words
(for each word in the vocabulary) in each document. Call this
$C^{DW}$, which can be read as ``a matrix of counts, with rows
corresponding to documents, and columns corresponding to words''. That is, the superscript indicates the ordering of the variables. I single entry in 
$C^{DW}$ could be referred to via $C^{DW}_{d,w}$

We assume each word, in each document, has an associated topic. But
this topic allocation is unknown: the topic index is a latent
variable. So the central data structure is going to be a {\bf matrix
  of counts} we can call $C^{DWT}$, indexed by document ($d$), word
($w$) and unknown topic ($t$). See Figure \ref{fig:3Dmatrices}.  

$C^{DWT}$ is the ``expanded'' form of $C^{DW}$, with each count
$C^{DW}_{d,w}$ now being distributed across the various possible
topics.  We don't know these topic allocations, but will start off
with random counts in $C^{DWT}$, subject to the sum over $t$ matching
the corresponding entry in $C^{DW}$.

For example, $C^{DW}_{d,w} = 11$ means that the \wth word occurs 11
times in the \dth document. If there are 3 topics, we might have
$C^{DWT}_{d,w,\bt} = (2,3,6)$, meaning that 2 of the 11 occurences
of the \wth word in this document are deemed to be due to topic 1, 3
from topic 2, and the other 6 from topic 3.

The arrow is shown going {\it from} $C^{DWT}$ {\it to} $C^{DW}$ in
Figure \ref{fig:3Dmatrices} just because summing up the counts
allocated to each topic gives you the original data counts. In the
figure, all the arrows denote summations of some kind.\footnote{There's something that bothers me about this: there would seem to be
    multiple paths (just ordering of summations!) to $C^T$ and $C^D$
    (e.g. could get a $C^D$ just by summing $C^{DW}$ over columns as
    well), but these would not match the ones shown here due to the
    the addition of $\alpha$ and $\beta$ on some paths but not
    others. What's that even {\it mean?!}}


\begin{figure}
\begin{center}
\includegraphics[width=.7\linewidth]{./pics/topic-model}
\caption{\label{fig:3Dmatrices} The matrices used in LDA. Direct
  counts from the data are grey, inferred ones involving topics are
  blue, and ``pseudocounts'' / hyperpriors are, erm, puse. The arrows
  represent summation if a dimension is being lost, and repeated
  addition if a dimension is being gained. }
\end{center}
\end{figure}



\subsection{Gibbs sampler}

Gibbs sampling is used to infer the latent variables (topic
allocations for words).  Here's how one ``iteration'' of Gibbs
sampling works.

For each $(d,w)$, do this $C^{DW}_{d,w}$ times, to mimic going through
each word in each document:
\begin{enumerate}
\item pick a topic $t$ at random with probability  $\propto C^{DWT}_{d,w,\bt}$
\item decrement $C^{DWT}_{d,w,t}$, $C^{DT}_{d,t}$, $C^{WT}_{w,t}$,
  $C^{T}_{t}$ and $C^{D}_{d}$. Thus we completely remove all the
  effects of one of the $C^{DWT}_{d,w,t}$ instances on the other
  statistics\footnote{This is like temporarily removing one of the word
  instances (together with its currrent topic assignment) from the
  corpus. This is required in order that the Gibbs Sampler can easily
  find the probability distribution for such a word over all topics,
  conditioned on all {\it other} information available on topics.}

\item Gibbs needs to sample from 
\begin{align*}
P(\bt \mid w,d) &= \frac{P(\bt, w \mid d)}{P(w \mid d)} \\
 &\propto P(\bt, w \mid d) \\
 &= P(w \mid \bt, d) P(\bt \mid d)  \\
 &= P(w \mid \bt) P(\bt \mid d) 
\end{align*}
So do this:
  \begin{enumerate}
  \item calculate the vector $P_{w|\bt} =
    \frac{C^{WT}_{w,\bt}}{C^T_\bt} $ (ie. element-wise division).

  \item calculate the vector $P_{\bt|d} = \frac{C^{DT}_{d,\bt}}{C^D_d} $. 
    
  \item calculate the vector that's the element-wise product of those two previous quantities: $\displaystyle P_{\bt | w,d} \propto P_{w|\bt}  P_{\bt|d} $.
  \end{enumerate}

\item choose a new $t^\prime$ from $P_{\bt | w,d}$.

\item increment $C^{DWT}_{d,w,t^\prime}$, $C^{DT}_{d,t^\prime}$, $C^{WT}_{w,t^\prime}$, $C^{T}_{t^\prime}$ and $C^{D}_{d}$.
\end{enumerate}


\section{Finding the mixture coefficients for a new 'document' \label{sec:newdoc}}

Nomenclature:
\begin{itemize}
\item $\data \equiv C^{DW}$ is the big dataset that we used to learn
  the LDA topics from.
\item $\theta$ is a matrix giving the distribution of words in each of
  our learned topics: $\theta_{w,t} = P_{w|t}$. (So this is the
  ``basement'' in the picture, after normalising per topic).
\item $\bc$ is the word counts for a {\em new} document: our model,
  consistent with that used to learn $\theta$, is that these arose by
  sampling from a mixture distribution over topics $t$ (followed by
  sampling from the word distribution of that $t$) although {\em we don't
  know the mixing coefficients.}
\item $\pi$ is those unknown mixing coefficients, which sum to 1. This
  is what we would like to infer.
\end{itemize}

Our problem: what do we think about the mixture of (known) topics that
were used to generate $\bc$?

\begin{align}
\overbrace{P(\pi \mid \bc, \theta, \data) }^\text{posterior}
&\propto \; P(\bc \mid \pi, \theta, \data)   \;\;\; P(\pi | \data, \theta)   \\
&= \; \underbrace{P(\bc \mid \pi, \theta)}_\text{likelihood}   \;\;\; \underbrace{P(\pi | \data, \theta)}_\text{prior}
\end{align}

The posterior is of course going to be a full distribution over
possible mixing coefficients. But I think we'd be quite content to
find the MAP $\pi$ for a given $\bc$, so it's the particular $\pi$
that would maximize the posterior that we want.

And for simplicity let's ignore the prior for now - it'll be some
Dirichlet thingy - ie. let's do maximum likelihood and try to find the
$\pi$ that would maximize $P(\bc \mid \pi, \theta)$. Hopefully it'll
be unique.

Since counts are generated independently (given some $\pi$) under our
model I think we can say
\begin{align}
P(\bc \mid \pi, \theta) &= \prod_w P(w \mid \pi, \theta)^{c_w}
\intertext{and its logarithm is}
\mathcal{L} &= \log P(\bc \mid \pi, \theta) \\
 &= \sum_w c_w \log  P(w \mid \pi, \theta) \\
\intertext{and the prob of a given word is a mixture over topic distributions (the $\theta$), isn't it? So this must be}
 &= \sum_w c_w \log  \sum_t P(w,t \mid \pi, \theta) \\
 &= \sum_w c_w \log  \sum_t \underbrace{P(t \mid \pi, \theta)}_{\pi_t}  \underbrace{P(w \mid t, \pi, \theta)}_{\theta_{w,t}} \\
% &= \sum_w c_w \log  \sum_t  \pi_t  \, P(w \mid t, \theta) \\
 &= \sum_w c_w \log  \sum_t  \pi_t  \; \theta_{w,t} \label{eq:likelihood}
\end{align}

So what normalised categorical distribution $\pi$ maximizes this
number?

\subsection{Just one word}
Reality check - this should give something sensible in the extreme
case of a ``one-word document''. In that case, $\bc$ will be all
zeroes apart from a single 1. Let's say it's the $w^\text{th}$  word.  Eqtn  \ref{eq:likelihood} will be
\begin{align}
\mathcal{L} &= \log  \sum_t  \pi_t  \; \theta_{w,t} 
\end{align}
so it will be maximized by putting all the topic distribution into the
one with the highest $P_{w|t}$. That seems extreme (essentially
``over-fitting'' by $\pi$) but not surprising - this is the likelihood
alone, as if there were no prior at all. 

Anyway, it's going to do what Anna did in the paper, in this case. So far, so good!

\subsection{Just two topics}

With just two topics, we have the advantage that the mixing coefficient for one is just 1 minus the coefficient for the other. Shirley, this has to be doable?  Eq \ref{eq:likelihood} becomes
\begin{align}
\mathcal{L}
% &= \sum_w c_w \log  \sum_t  \pi_t  \; \theta_{w,t} \\
 &= \sum_w c_w \log  (\pi_0  \; \theta_{w,0}  \; + \; \pi_1  \; \theta_{w,1}) \\
 &= \sum_w c_w \log  ((1-\pi_1)  \; \theta_{w,0}  \; + \; \pi_1  \; \theta_{w,1}) 
\intertext{and the gradient $G=\frac{\partial \mathcal{L}}{\partial \pi_1} $ is}
G(\pi)  &= \sum_w c_w \bigg[ \frac{\theta_{w,1}  - \theta_{w,0} }{ (1-\pi_1)  \; \theta_{w,0}  \; + \; \pi_1  \; \theta_{w,1} } \bigg]
\intertext{I can't see an analytic solution for the $\pi$ that would make this zero. However, Newton's Method is an algorithm for finding a zero, via an iterative process:}
\pi_1^\text{new} &= \;\pi_1 \;\; - \;\; \frac{G(\pi_1)}{G^\prime(\pi_1)} \label{eq:Newton}
\intertext{(one can see that, provided this converges, the numerator $G(\pi_1)$ in \ref{eq:Newton} must arrive at zero, which is what we want).}
\intertext{For Newton's method we'll also need the second derivative $G^\prime=\frac{\partial G}{\partial \pi_1} $, which (couple of lines omitted) rather conveniently turns out to be}
G^\prime(\pi_1)  &= -\sum_w c_w \bigg[ \frac{\theta_{w,1}  - \theta_{w,0} }{ (1-\pi_1)  \; \theta_{w,0}  \; + \; \pi_1  \; \theta_{w,1}} \bigg]^2 
\intertext{So in our case the Newton's update is:}
\pi_1^\text{new} &= \pi_1 \;\; + \;\; \frac{\sum_w c_w  \, \beta_w}{\sum_w c_w  \, \beta_w^2}\label{eq:newton2topicsUpdate}
\intertext{where}
\beta_w &= \frac{\theta_{w,1}  - \theta_{w,0} }{ (1-\pi_1)  \; \theta_{w,0}  \; + \; \pi_1  \; \theta_{w,1}} \label{eq:newton2topicsBeta} 
\end{align}

So an algorithm is to take the current $\pi_1$ and calculate $\beta_w,
\, \forall w$, as per \ref{eq:newton2topicsBeta}. Then update $\pi_1$
to a better value as per \ref{eq:newton2topicsUpdate}. Repeat until
$\pi_1$ converges. Finally, we might check it's $\in \left[0,1\right]$.

That seems quite appealing to me.  Worth checking, anyway.




\subsection{More than two topics}
The issue with more topics is that changing one coefficient means
changing all others to keep the sum unity, but unlike the 2-topics
case it's not immediately obvious how to change all at once.

One way might be to use Lagrange multipliers, with constraints that
$\sum_t \pi_t=1$ and $\pi_t \geq 0\; \forall t$. 

Another way (and my preference) is to build the constraints in, by the
``softmax'' reparameterisation, as follows. Define
\begin{align}
 \pi_\tau &= \frac{e^{u_\tau}}{Z} \;\;\;\;\; \text{where }\; Z =
 \sum_t e^{u_t} \intertext{which builds in the normalisation and
   positivity and leaves $u$ free to roam over the reals.  We will need the
   gradient of $\pi$ w.r.t. $u$, which is:} 
\frac{\partial \pi_\tau}{\partial u_t} &= \frac{e^{u_\tau}}{Z} \delta_{\tau,t} \;\;-\; e^{u_\tau} Z^{-2} \frac{\partial Z}{\partial u_t} \\ 
&= \frac{e^{u_\tau}}{Z} \delta_{\tau,t} \;\;-\; e^{u_\tau} Z^{-2} e^{u_t} \\ 
&= \pi_\tau (\delta_{\tau,t} \;\;-\; \pi_t)
\end{align}
where $ \delta_{\tau,t}$ is a delta function at $\tau=t$.

So to recap, we've got 
\begin{align}
\mathcal{L}_\bc & = \log P(\bc \mid \bu, \theta) 
= \sum_w c_w \log  \sum_\tau   \; \theta_{w,\tau} \; \pi_\tau
\intertext{and the gradient of that with respect to $u_t$ is going to be}
G(\bu) &= \frac{\partial \mathcal{L}_\bc}{\partial u_t} \\
&=  \sum_w c_w \log \bigg[ \sum_\tau   \; \theta_{w,\tau} \; \pi_\tau (\delta_{\tau,t} \;\;-\; \pi_t)  \bigg] \\
&=  \sum_w c_w \log \bigg[ \theta_{w,t} \; \pi_t \;\;\; - \;\;\;  \pi_t \sum_\tau  \; \theta_{w,\tau} \; \pi_\tau  \bigg]  \\
&=  \sum_w c_w \log \pi_t \bigg( \theta_{w,t}  \; - \;  \sum_\tau  \; \theta_{w,\tau} \; \pi_\tau  \bigg)  
\end{align}

I guess we'd like to ensure this gradient is zero by solving
\begin{equation}
 \sum_w c_w \log \bigg[ \pi_t \bigg( \theta_{w,t}  \; - \;  \sum_\tau  \; \theta_{w,\tau} \; \pi_\tau  \bigg) \bigg] = 0 \label{eq:gradient} 
\end{equation}
for all $t$.

Solving that family of equations - one like Eq \ref{eq:gradient} for
each topic - amounts to finding the most likely proportions of each
topic in an arbitrary new document (eg. a mini-window). I note that
\ref{eq:gradient} is really pretty ugly!

Q: is there a Newton's Method for vectors??


\Line


Q: or... what other ways are available to optimize Eq
\ref{eq:likelihood}?  If Eq \ref{eq:likelihood} has just one maximum
then this is a {\it convex optimization problem} and there are
certaintly clever tricks... Paul Teal knows some!


\section{LDA for detecting 'blobby' regions of one topic in images \label{sec:images}}

For images, words in a vocabulary correspond to (binned) pixel
intensities, documents are windows\footnote{in practice, so far,
  they're subwindows in a single large image, but whatever.}, and the
topics we're interested in are just the ``background'' and ``the
rest''. Background is simply the topic that shows up as dominant when
you look at enough windows that are large enough. Of course, we expect
that this will correspond to the darkest pixels, but don't commit to
that {\it a priori}!

We know (i.e. ``want to assume''!) something that's not in LDA for
text documents here: regardless of the pixel values typical of sources
in images, we believe those sources to be spatially extended
contiguous regions in the image.

Suppose we've run LDA on a big image / lots of other images and
acquired $P_{w|t}$ for a set of topics. For simplicity, say there are
two: $t_0$ is the the ``background'' topic, and $t_1$ is all the rest,
aggregated: ``source''.
We want to find a large, contiguous region that's suspiciously $t_1$.

Generative model: 
\begin{itemize}
\item there's a source centered at $\mu$ in the image, with ``spread'' determined by a positive definite matrix $\Sigma$.

\item For the \ith pixel, at position $\bx_i$ in the
image, and having intensity $w$ (since that's the ``word''), we 
\begin{enumerate}
\item choose a topic:
\[ P(t=t_1) = \exp \bigg( -\half (\bx-\mu)^T \Sigma^{-1} (\bx - \mu) \bigg)
\]
This is a ``Gaussian blob'' shaped region. At the middle (vector position $\mu$) we will always choose $t_1$, but far from there we'll always choose $t_0$.
\item choose a word given that topic, from $P_{w|t}$.
\end{enumerate}
\end{itemize}

With that as the likelihood model, we go in search of optimal regions, by using some criterion to optimize vector $\mu$ and matrix $\Sigma$.

\subsection{what criterion should we use?}

\subsubsection{first: clarify connection between likelihood and KL in general}
Say we have a ground truth distribution $P(\bx)$ that is a density
over some $d$-dimensional vector space $\bx \in X^d$. And we have a
model distribution $Q_\phi(\bx)$ over this same space, with parameters
$\phi$.  The KL divergence is
\begin{align}
KL(P || Q_\phi) &= \int P(\bx) \log \frac{P(\bx)}{Q_\phi(\bx)}  \; d\bx\\
&= \int P(\bx) \log P(\bx) \, d\bx \;\; - \;\; \int P(\bx) \log Q_\phi(\bx) \, d\bx
\end{align}
Minimizing the KL divergence in terms of $\phi$ is the same as
maximizing the second term (because the first term doesn't even
involve $\phi$).  

{\bf We don't actually know $P$ however}: instead we have
a bunch of samples taken from it - statisticians might call this the
``empirical distribution'', $\tilde{P} \equiv \{\bx_i\}_{1..n} $, ie. a
set of Dirac delta functions, in place of the true continuous
distribution.

To the extent that $\tilde{P}$ approximates $P$, we can do the
integral in the second term:
\begin{align}
\int P(\bx) \log Q_\phi(\bx)\, d\bx \; &\approx \int \tilde{P}(\bx) \log Q_\phi(\bx)\, d\bx  \\
&= \; \sum_i \log Q_\phi(\bx)
\end{align}

{\bf Thus maximizing the likelihood of the model is the same (up to the
  approximation) as minimizing the KL divergence between the true
  distribution and the model one}.

\subsubsection{what about the 'reverse' KL?...}

This would appear to be
\begin{align}
KL(Q_\phi || P) &= \int Q_\phi(\bx) \log \frac{Q_\phi(\bx)}{P(\bx)}  \; d\bx\\
&= \int Q_\phi(\bx) \log Q_\phi(\bx) \, d\bx \;\; - \;\; \int Q_\phi(\bx) \log P(\bx) \, d\bx 
\end{align}
Minimizing the first term means maximizing the entropy of the $Q$
distribution, so this term encourages $Q$ to be as broad as
possible. The second term is the ``interesting'' one, since we don't
have a full $P$ distribution, only $\tilde{P}$. Since $\tilde{P}$ is
delta functions in a sea of zero, and $\log(0^+) \to -\infty$ we have
a problem!

So I cannot see how this, desirable as it may be, can actually be
calculated if you have a parameterised $Q$ rather than an ``empirical
distribution'' one, and an ``empirical'' $P$ rather than a functional
one. That's regretable.

\section{Something that seems to work}

.... {\it I'm left wondering how the heck my blob detection scoring scheme worked}, since at least in my mind at the time it was ``inspired by'' the reverse KL divergence.

The score I used seemed to work remarkably well in the Gaussian noise case I tried.
The score was this: 
\begin{align}
KL_\text{ish} &= \frac{\sum_i  w_i \log z_i}{\sum_i  w_i} \label{eq:reversedKLish}
\intertext{where}
w_i &= \exp(-\half (x-m)/r)
\end{align}
where $i$ indexes pixels. $m$ is the center of a region whose extent
is controlled by $r$. The denominator is essentially a hack that
compensates for the data that would exist but is ``lost'' beyond the
boundaries at 0 and 1000.  But this (eqtn \ref{eq:reversedKLish}, and
ignoring the denominator for simplicity) looks nothing like a KL
divergence really. $z$ is the {\it intensity}, not a probability at
all, and $w$ is a weighting function, not a model distribution,
methinks.

Still, it ``works'':\\
\includegraphics[width=0.25\linewidth]{./pics/test_ground}
\includegraphics[width=0.25\linewidth]{./pics/test_image}
\includegraphics[width=0.25\linewidth]{./pics/test_contours}\\
First column is ground truth, second is the noisy image made from that, and third is the contours of the score function.

But we're just taking a local average of $\log z$  - this is not rocket
science!  A good thing about $\log z$ is that multiplicative scaling of $z$
has no effect whatever.  On the other hand adding a constant to $z$
DOES affect the score - but it still won't affect the position of the
{\it maximum} and so I guess that's not so bad.  

But this is all heuristically argued and one suspects that other
similar schemes might work just as well. So I tried a couple of others
out (just on a 1-dimensional ``image'' this time):
\begin{align}
\text{RED:} & \sum_i w_i \log z_i \\
\text{BLUE:} & \sum_i w_i z_i^2 \\
\text{YELLOW:} & \sum_i w_i z_i \\
\end{align}
\includegraphics[width=\linewidth]{./pics/testKL}\\ The data (dots) are
sampled i.i.d. from a standard\footnote{mean zero, unit variance.}
Gaussian distribution, except in the region of the grey curve where
the mean rises to one, smoothly.  The curves show the score obtained
as one varies the middle of the window $m$ across the image.  For each
colour, the different curves used for different widths used in the $w$
weighting function.

Essentially {\it anything works}!!! All you have to do is average {\it
  something} over a local region, i.e. all that matters is that you do
some smoothing / low-pass filtering of the image, and then pick the
max points.

But that's just trivial and is not a novel contribution! Whatever we
come up with, it would be nice if it could beat at least this straw
man.

But it does serve to sharpen the issue: if we're not just after local parts of the image that have heightened mean intensity, then what are we really after??!

Here's an image in which the ``object'' has the {\it same} mean intensity, but just from a ``tighter'' distribution:\\
\includegraphics[width=\linewidth]{./pics/devils_advocate_skyscape0}\\
Everything's useless. Maybe RED has a clue?

Let's make it easier by having a higher-than-average mean, but also a more narrow distribution: \\
\includegraphics[width=\linewidth]{./pics/devils_advocate_skyscape1}\\
Useless.

And this is the same but from a ``looser'' distribution:\\
\includegraphics[width=\linewidth]{./pics/devils_advocate_skyscape2}\\
Fairly crappulous here too, although BLUE might be onto something.

The point is that it's more than a bit hit and miss! That's what you
get when you live in Heuristicville Arizona. Wall-to-wall cowboys.

We should really be looking for regions that are {\it not like the
  background, in an unspecified way}, but we keep ``specifying'' it,
so it keeps biting us in the rear.

Suppose we get serious about this notion of ``source'' being not specified as anything except {\it not the background}. We will have to start with a notion of background distribution that {\it is} well specified. 

So how do you evaluate how ``not'' something is?  Having a null
hypothesis and rejecting it seems half sensible, but (a) the test we
tried based on that didn't work at all well, and (b) what would a
Bayesian solution to this question look like?


\section{Bayesian model comparison for locating sources}

[NOTE TO MARCUS: The LDA model is that the ``background" and ``source" distributions are fixed multinomial distributions which are both drawn from the same Dirichlet distribution (with the same $\alpha$ vector); whereas here I think we are saying that each ``topic" distribution is drawn from its own Dirichlet distribution (with its own $\alpha$ vector). Therefore we could specify that the ``background" and ``source" distributions are not fixed but can/do vary over the image. For example, we might draw a background distribution and a source distribution for each sub-image in an image, from their respective Dirichlet distributions (and from these then draw the actual pixel values in the sub-image). This is like the Dirichlet compound multinomial extension LDA in Doyle \& Elkan (2009) and Madsen, Kauchak \& Elkan (2005) in their ``modelling word burstiness" papers. [see section 4.2 of my ``done\_to\_date.pdf" from May/June for a short description of DCMLDA + full references for the two burstiness papers]. ALSO: when we move on from assuming a fair coin toss for the mixing proportions of source \& background for an image \ sub-image I guess we would have a Dir dist from which to draw these proportions per sub-image.]

An image (and its sub-images) can be modelled as a distribution over
pixel intensities. Radio astronomy images can be thought of as
primarily background with an unknown number of spatially extended
sources; and so these images can be thought of as a mixture of
``background" and ``source" distributions. Continuous pixel intensity
values in astronomical images can be discretised by ``binning" them
over $K$ pixel intensity ranges. These bins needn't necessarily be
equally spaced over the pixel intensity range, but may be constructed
using a different strategy, for example bins that are distributed
across the pixel intensity range so that there is equal occupancy in
each bin.

Rather than modelling an image as a mixture of two fixed multinomial
distributions; we might want to incorporate the variablity of the
background and source distributions into our model (?) and therefore
use Dirichlet distributions as our background and source
distributions. For any particular sub-image fixed background and
source multinomial distributions can be drawn from their respective
Dirichlet distributions.

Dir Mult distribution is... what it is.  Has hyperparameters
$\alpha_{1..K}$.
For some vector of counts $\bn$ in $K$ bins of a histogram,
integrating out the multinomial distributions that result from the
Dirichlets gives the likelihood:
\begin{align}
P(\bn|\alpha) &= \frac{\Gamma(A)}{\Gamma(N+A)} \prod_k \frac{\Gamma(\bn_k+\alpha_k)}{\Gamma(\alpha_k)}  
\label{eq:muldir} 
\intertext{where}
A &= \sum_k \alpha_k \\
N &= \sum_k \bn_k
\end{align}

The logarithm of this is: 
\begin{align}
\log P(\bn|\alpha) &= \log \Gamma(A) - \log \Gamma(N+A) + \sum_k \log \Gamma(\bn_k+\alpha_k) - \log \Gamma(\alpha_k) \label{eq:logmultdir}
\end{align}

The thing we're interested in (a ``score'' if you like) is the ratio of posterior probabilities under the two models (assuming an equal mixture of background and source in any particular sub-image for now). And we'll deal in logs of this. So the idea is that
\begin{align}
\text{Score} &= \log \frac{P(\alpha^S | \bn)}{P(\alpha^B | \bn)} \label{eq:score} \\
&= \log \frac{P(\bn | \alpha^S)}{P(\bn | \alpha^B)} \;\; + \; \log \frac{P(\alpha^S)}{P(\alpha^B)}
\end{align}

Equations \ref{eq:score} and \ref{eq:logmultdir} enable us to do
fully Bayesian model comparison between any two Dirichlet-Multinomial
distributions for the observed pixel values then.


Given that the number of background pixels in such an image far
outnumber the source pixels, and that the variability among background
pixels is lower than that for source pixels, we might expect the
$\alpha^B$ vector for the background distribution $B$ to be large and
relatively uniform, when compared to the $\alpha^S$ vector for the
source distribution $S$, which contains much smaller values. We use
positive values for both $\alpha$ vectors for convenience's
sake. [Note that if some bins are more likely than others overall/in
  the whole image for a particular topic, these may be given higher
  values in the corresponding alphas].

\subsection{approximating the score leads to a difference of entropies}

It is easy enough to calculate the score that follows from equations
\ref{eq:score} and \ref{eq:logmultdir} directly, but it's interesting
/ illuminating to simplify too.

For positive numbers, it turns out that $\log \Gamma(x) \approx x \log
x - x $.  This approximation is ``pretty good'' (Frean 2012,
pers. comm.) for $x>3$. Here's a pic:
\\ \includegraphics[width=.7\linewidth]{./pics/approx_to_gamma}

Plugging in this approximation, the log likelihood
\begin{equation}
log P(\bn|\alpha) = A \mathcal{H}(\alpha) - (N+A)\mathcal{H}(\bn+\alpha)
\end{equation}

The entropy  of a categorical distribution is
\begin{equation}
\mathcal{H}(p) = - \sum_{k=1}^K p_k \log p_k \geq 0
\end{equation}
which is $\leq \log K$, with equality if and only if $\bp$ is uniform.

which simplifies (how?) to

\begin{equation}
\log \frac{P(\bn|S)}{P(\bn|B)} = A^S\mathcal{H}(\alpha^S) - (N+A^S)\mathcal{H}(\bn+\alpha^S) - A^B\mathcal{H}(\alpha^B) + (N+A^B)\mathcal{H}(\bn+\alpha^B) \approx N\mathcal{H}(\alpha^B)
\label{eq:approxscore}
\end{equation}

So the score is approximately
\begin{equation}
\approx N(\mathcal{H}(\alpha^B) - \mathcal{H}(\bn))
\end{equation}

(and $\mathcal{H}(\alpha^B)$ is equal to $log K$ if bins have equal occupancy).

So far this is for general $\alpha^S$ and $\alpha^B$, but we assume
the latter is large compared with the counts in a region of interest,
and the former is small.

In the particular case of a uniform prior (ie. eq occ bins) the
entropy of $\alpha^B$ is just $\log K$.

\subsection{eg}

Figure \ref{fig:DirichletMultinomialModelComparisonExample}  is an example.

\begin{figure}
\begin{center}
\includegraphics[width=.75\linewidth]{./pics/DirModelComparison_data_69}
\includegraphics[width=.75\linewidth]{./pics/DirModelComparison_score_69}
\caption{\label{fig:DirichletMultinomialModelComparisonExample} 
ground truth on the triangle is the circle. we see the model being penalised both for getting the region too small (N is smaller so it gets hurt for that) and too large (N larger, but this is outweighed by the 'more uniform' $n$ that results).
}
\end{center}
\end{figure}

QUESTION: LOOKS LIKE SCORE CAN'T EVER GO NEGATIVE, BUT WHY NOT? HAVE i STUFFED UP MATH? SEEMS ODD..... SURELY THERE ARE WINDOWS WHERE i STRONGLY PREFER HYPOTHESIS THAT IT IS BACKGROUND, SO 'SCORE' SHOULD BE NEGATIVE THERE...............?

nb. This is a model which explicitly does {\it not} model sources, in the
usual sense. Saints Osho and Krishnamurti would approve of the
backwards-ness of this.

\subsection{softening the boundaries}

We can think of the hard boundary case as one way of coming up with counts:
\begin{align}
n_k &= \sum_i \delta_{b_i=k} \; w_i(\theta) 
\end{align}
where $i$ indexes pixels in the image, and $b_i$ is the bin index for
the \ith pixel, and $\delta$ is the delta function. An all-or-nothing
$w$ defines a set of pixels over which the sum will ``count''. $w$ is
parameterised by some numbers $\theta$, for instance
$\theta=(\bm,\br)$ could be vectors specifying the middle and the
width of a contiguous region in the image, with $w_i(\theta)=1$ if the
\ith pixel at position $(x_i,y_i)$ is within a ``box'' $| x_i - m_{x}|
< r_x$ and $| y_i - m_{y}| < r_y$. This is just a 2-dimensional
version of the 1D example above then.

Generalising, we might instead make $w$ a smooth function of position
in the image.  This better reflects our belief that sources don't
suddenly go from absent to present in the space of one pixel. It also
has the computational niceity that we can calculate gradients
w.r.t. $\theta$ and use those to perform local search
a.k.a. optimization of the region: ``source finding''.

A suitable smooth function might be
\begin{align}
w_i(\bm,\br) &= \exp \left[- \half (\bx - \bm)^T \bC^{-1}(\bx - \bm) \right]
\intertext{with }
\bC &= \begin{bmatrix} r_x & 0 \\ 0 & r_y \end{bmatrix}
\intertext{for an ``axis aligned'' ellipse, and I guess}
\bC &= \begin{bmatrix} \cos\phi & -\sin\phi \\ \sin\phi & \cos\phi \end{bmatrix}\begin{bmatrix} r_1 & 0 \\ 0 & r_2 \end{bmatrix} \\
 &= \begin{bmatrix} r_1 \cos\phi & -r_2 \sin\phi \\ r_1 \sin\phi & r_2 \cos\phi \end{bmatrix}
\intertext{for a rotated one.}
\end{align}
So in this case the parameters will be $(r_1, r_2, \phi)$.

The gradient of the approximate score w.r.t. some parameter $\theta$ would seem to be
\begin{align}
\frac{\partial}{\partial \theta} \text{Score} &= \frac{\partial}{\partial \theta} \big( - \, N \,  \mathcal{H}(\bn) \big)\\
&= - \, \frac{\partial N}{\partial \theta} \, \mathcal{H}(\bn) \; - \, N \frac{\partial \mathcal{H}(\bn)}{\partial \theta}
\end{align}
So we've got two derivatives to figure out.

I think that the gradient of the entropy of the counts $\bn$ would be 
\begin{align}
\frac{\partial \mathcal{H(\bn)}}{\partial \theta} &= \sum_k (1+\log n_k) \frac{\partial n_k}{\partial \theta}
\intertext{and that}
\frac{\partial n_k}{\partial \theta} &= \sum_{i \text{ for which } b_i=k} \frac{\partial w_i(\theta)}{\partial \theta}
\intertext{(cumbersome notation! must be a better way...) so we could write the first thing as a sum over all pixels,}
\frac{\partial \mathcal{H(\bn)}}{\partial \theta} &= 
\sum_i (1+\log n_{k(i)}) \frac{\partial w_i(\theta)}{\partial \theta}
\end{align}
where $k(i)$ is the bin index of the \ith pixel.

{\em ..Is that right???}

I think the other gradient is simpler:
\begin{align}
\frac{\partial N}{\partial \theta} &= \sum_i \frac{\partial w_i(\theta)}{\partial \theta}
\end{align}


So now we have to come up with $\frac{\partial w_i(\theta)}{\partial
  \theta}$ for that rotated ellipse. Is that going to be hard?

Then we plug that in and we'll have the gradient of the score
w.r.t. the parameters of $w$, which I think is what we need in order to ``fit'' blobs to non-backgroundy pixels.




\section{discretization strategies (a.k.a. setting the bin boundaries)}
Binning is discretization.

\subsection{Equal widths}
\begin{itemize}
\item good: simple, intuitive.
\item bad: lots of empty bins: wasted representation power.
\item bad: lack of resolution at the low end. 
\end{itemize}

Thinking of it as a coding scheme, to be judged on how well originals
are recoverable: this as a representation that is poorly invertible
{\it in the sense that fine details of low values are all
  lost}. I imagine it does an optimal job of minimizing the rms error in
the recovered values. Do we care about that?

\subsection{Exponentially increasing widths}
This still doesn't seem to be ``enough'' of a stretching of space to
give sufficient resolution in the low values.

\subsection{Equal occupancies}
ie. each bin is used exactly the same amount.
\begin{itemize}
\item good: no wasted bins - every bin conveys as much information about where a pixel is in the RANKING of intensity values.
\item good: simple in how it plays out in the Score we're exploring lately.
\item bad: lack of resolution at the high end. (solution= more bins?!).
\end{itemize}

Another representation that is poorly invertible, but this time {\it
  in the sense that values (even ball-park) of high values are
  lost}. I think it does an optimal job of preserving {\it ranking},
but is this something we should care about?


\subsection{Exponentially decreasing occupancies}
ie. each bin is used $1>\gamma>0$ times as much as the one before.
The bottom bin will be used a lot. The second will be used (say) half as much. And so on... The top one will be rarely utilized at all.

If $\gamma=1$ it's the Equal Occupancy strategy.

With $\gamma=\half$ the entropy will be:
\begin{align*}
\mathcal{H}(\bp) &= \sum_i^n p_i \log \frac{1}{p_i} \\
&= \frac{1}{2} \log 2 \, + \, \frac{1}{4} \log 4 \, + \, \frac{1}{8} \log 3 \, + \ldots 
\intertext{In the limit of lots of bins this is}
\lim_{n\to \infty}\mathcal{H}(\bp) &= 2
\end{align*}
How about that! The entropy is exactly 2, in the limit. It approaches it reasonably quickly, e.g. for 10 bins it is 1.988.

\section{`soft' binning?}
Problem: For our approach we need the incoming real-valued intensity
values to be discretized or "binned", but there are 2 problems with
binning: 
\begin{enumerate}
\item we don't have any informed basis for deciding on what bin
boundaries to use and so any particular choice amounts to an
unjustified assertion, {\it and}...
\item discretization results in sudden
changes at the borderline values, which introduces variation into the
system that wasn't there in the original.
\end{enumerate}

But all we really need bin identities {\it for} is to add up the counts in
some region.  When a new value comes along, we might be better off
including "proportions of it" in several nearby bins, not just
one. 

One could (and we tried to...) make up some ad hoc scheme that would
``give'' some partial counts to neighbouring bins of the ``true''
one. But that's not satisfying. How to do this in a sensible, robust way?

Idea: if we don't know which bins to use, maybe we should make up
heaps of them. Then for a given pixel intensity value x, we can have a
histogram of "possible" bin assignments instead of a single one.


Suppose we give up on the truth (seems sensible as we don't know it!)
and make heaps of plausible sets of bin borders. Then, for a given
intensity value $z$ we can count how often it appears in bin 5,6,7 etc
- this gives a {\it distribution} over all the bins, which reflects
our uncertainty about where the borders should lie. This will solve
the problem of the sudden changes at bin borders. Essentially it's a
way to convert floats into distributions over discretes (counts) in a
way that is smooth.

A nice way to make those borders is just to sample from our best
friend the Dirichlet distribution then, $\Dir(\alpha_{1..K})$.  The
Dirichlet distribution gives a way to do this, since it produces a set
of numbers that automatically sum to one (or whatever the relevant
range is to be). Most appealingly, its parameters $\alpha_{1..K}$ enable one
to control both the typical widths of bins (our intuitions/whatever
about what their relative sizes should be) and their variation
(ie. our uncertainty about precisely where they should lie).

Figure \ref{fig:test_DirBins} shows examples.

\begin{figure}
\begin{center}
\includegraphics[width=.49\linewidth]{./pics/test_DirBins_s1_a250.png}
\includegraphics[width=.49\linewidth]{./pics/test_DirBins_s1p3_a250}
\caption{\label{fig:test_DirBins} On the left, all the $\alpha$
  parameters are the same.  On the right, $\alpha_{i+1} = \beta \times
  \alpha_{i} $, with the stretch factor $\beta = 1.3$. Their sum $A
  =\sum_i\alpha_i$ is set to 250 in both cases.  (Figures made with
  binning-by-Dirichlets.py).}
\end{center}
\end{figure}

\subsection{How to choose the stretch factor?}
Anna points out the parameters are piling up here. WHich is true.

Suppose we've got some set of pixels, with intensity values forming a
set $\{z_j\}_{j=1..n}$.

One way to set the stretch factor parameter is to empirically choose
the one that leads to the {\it most even spread of bin counts}
overall, by maximizing the associated entropy. Figure
\ref{fig:test_entropy} shows an example on a seriously skewed
distribution made like this:
\verb+test_vals = np.power(rng.random((1000,)), 5.0)+

\begin{figure}
\begin{center}
\includegraphics[width=.69\linewidth]{./pics/test_DirBins_entropies}
\caption{\label{fig:test_entropy} Top: a really skewed distribution of
  pixel intensities, perhaps something like the ones we encounter in
  radio astronomy images.  Below: entropy calculated for various
  values of the stretch factor. (but b.t.w. I don't trust the peak at
  2 in fact - suspect we're losing precision due to having lots of
  bins, $2^{-100}$ being quite a small number...!)}
\end{center}
\end{figure}


\subsection{Anna's idea}
Okay so these $\Dir(\alpha)$ samples are giving us partitions over
$[0,1]$, which Marcus was thinking of as the space of the pixel
intensities itself. Anna points out we could think of them instead as
alternative places in a {\it ranking} of $\{z_j\}_{j=1..n}$.

So, for each sample partition, $\alpha_{i=1..K}$, the \kth value will
have two associated items in $\{z_j\}_{j=1..n}$ : the one just above
and the one just below that real-valued point the discrete
ranking. It's just floor and ceil of $\alpha_k \times n$, I guess! The
right bin border value could be just a linear interpolation between
the two, I suppose.

So.... we'd convert each $\alpha_{i=1..K}$ vector into a vector of
actual bin boundaries in this way.

And then, if we use $\beta=1$ (no stretching) we'll get perturbations
around the theme of equal occupancy.

That is so neat!

Whereas in my initial scheme, the ``default'' value of $\beta = 1$
leads to equal bin widths in the limit of large $A$, in this new
scheme $\beta = 1$ leads to equal bin {\it occupancies}. And I've
always claimed the latter was a better default\footnote{Possible
  conceivable issue: might be somewhat fragile if the set $\{z_j\}$
  isn't that large??  But that's a scenario we're trying to avoid
  anyway.}. 

Running \verb+python annas-idea-bin-by-Dirichlets.py 0.5 100.0+ gives
this (below), for example. The \verb+0.5+ is the ratio of the last bin
to the first one. The \verb+100.0+ is the sum of the $\alpha$ vector.
Note that the test values being used are heavily skewed, as in the
previous example, but are also shifted and stretched so their min
value is -1 and their max is about 21 I think. Anyway:
\begin{verbatim}
...
a few internal_bin_borders derived from applying those to test_vals:
[[ -0.9885  -0.8838  -0.3156   1.1223   2.9392  11.0985  18.5827]
 [ -0.9922  -0.8989  -0.6068   0.8159   2.3571   6.8738  11.3315]
 [ -0.9997  -0.9887  -0.7658  -0.0239   1.4336   3.3415   8.1631]
 [ -0.9992  -0.9014  -0.1701   1.333    3.4101   8.2281  16.3029]]

an example of bin attributions, for an input of -0.807: 
[  0  30 426  44   0   0   0   0]

total bins counts over all the test_vals: 
[86894 77357 70426 64309 57851 52989 48031 42143]
\end{verbatim}
Notice how the \verb+0.5+ we gave it has resulted in the last bin having half as many occupants in it as the first.

So we still are interested in finding a ``good'' value for this ratio. Lord knows what the right one is.
But the hope is certainly that the inference algorithm is {\it reasonably} robust to this choice, especially now that we're handling the diversity of possible bin choices better.

Similarly there's a sweet spot for the total over $\alpha$ too, isn't there. 

\Line
\end{document}
