\chapter{Latent Dirichlet allocation}\label{C:LDA}

In this chapter, the topic modelling technique latent Dirichlet allocation \cite{blei2003latent} is used to infer multinomial models for background and source in discretised images. These models are used to segment the images, in order to identify sources.

\section{Problem framework}
As described in Chapter \ref{C:intro}, radio astronomy images can be thought of as primarily background with an unknown number of point and spatially extended sources. Identifying the sources requires distinguishing them from background, a task made difficult by the diversity within and between sources. The variability of background is lower than that of sources and in that sense it is easier to identify. 

Source detection may be approached as a problem of identifying and excluding regions of background and merging what remains into a modest number of sources. This requires specification of a method for labelling a region as likely or unlikely to be background.

One plausible approach is to assume background is equivalent to ``non-signal" with some noise, and this is the basis for many existing algorithms \cite{masias2012review}. However, this assumption can fail. Background pixels may not be restricted to a single narrow range of pixel intensities, or to just one such band, and may lie in source intensity ranges \cite{norris2011emu}. 

A second possible approach is to use a human domain expert to identify ``valid" background regions, and use this to build a probabilistic model for segmentation. However, such manual intervention is problematic given the volumes of data to be produced by next generation telescopes \cite{norris2011emu}, and is vulnerable to biases of the human perceptual system.

In this chapter, a method for producing models of background and source without manual intervention is proposed, where the models are extracted from image data containing both background and sources. This task is nontrivial: individual pixels in an astronomical image are not spatially independent (source pixels are more likely to be found with other source pixels, and similarly, background pixels are more likely to be found together than with source pixels), but regions of the image may contain only background pixels, only source pixels, or an unknown mixture. This motivates the use of the ``mixed-membership model" latent Dirichlet allocation (LDA) \cite{blei2003latent}. 

Though the problem of finding sources in astronomical images is framed as one of building a good model of background (so as to identify non-background --- i.e., source --- regions), LDA allows background and source models to be built concurrently \cite{blei2003latent}.

Source detection in radio astronomy images is performed via flood-filling, based on a probabilistic model of pixel intensities inferred by LDA. An additional application of this technique in segmenting greyscale images is presented. 

\section{Latent Dirichlet allocation}\label{sec:LDA}

\subsection{The generative model}

A ``topic model" is a generative model for documents\footnote{A ``generative model" is a description of a probabilistic procedure for generating documents, used to form a conditional probability density function and infer the latent topics (rather than actually generate documents) \cite{steyvers2007probabilistic}.} based on latent topics, where topics are modelled as distributions over a vocabulary of words and documents are modelled as mixtures of topics \cite{steyvers2007probabilistic}. 

Informally, a topic can be thought of as a ``semantic theme". The words that have high probability under a particular topic would be expected to convey some sense of meaning \cite{blei2007supervised}. For example, words like ``environment", ``conservation", ``sustainable", and ``ecology" found together with high probability in a particular topic might convey a theme of ``environmental sustainability".

Topics are discovered by fitting the generative model to data and finding the best set of latent variables to explain the observed data; for example, the best mixture of topics in a document and distributions of words in a topic \cite{steyvers2007probabilistic}.

LDA \cite{blei2003latent} is one such generative probabilistic model for sets of discrete data such as collections of documents, where a document is a multinomial distribution over topics, and topics are multinomial distributions over the vocabulary of words in the collection\footnote{Note that categorical distributions would be most appropriate for distributions over words in topics and distributions over topics in documents; however the multinomial distribution is used for its convenient properties, including its conjugacy with the Dirichlet distribution (and in fact a $K$ dimensional categorical distribution is equivalent to one trial of a $K$ dimensional multinomial distribution;\cite{blei2003latent,diaconis1979conjugate,kuettel2010what}.}.

Each document in a collection of documents is represented as a ``bag of words" in LDA. Under the generative model described by LDA, a document is generated by:
\begin{enumerate}
\item drawing topic proportions for that document; and then
\item given the document's topic proportions, for each word that will be in the document:
\begin{enumerate}
\item drawing the topic for that word;
\item generating the actual word by drawing it from the distribution corresponding with its assigned topic \cite{blei2011introduction}.
\end{enumerate}
\end{enumerate}

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{IMAGES/tm-graphical.png}
\caption[Graphical model for LDA]{\textbf{Graphical model for LDA.} Nodes are random variables (shaded are observed; unshaded are latent), directed edges show dependence. Boxes denote replication. Image adapted from \protect\cite{blei2009topic}.}
\label{fig:tm-graphical}
\end{figure}

More formally, and with reference to the graphical model in Figure \ref{fig:tm-graphical}, the generative model assumes there are $K$ topics, each $\beta_k$ of which is a multinomial distribution over all of the words in the collection of documents (the collection's vocabulary). The topic distributions are drawn from a Dirichlet distribution with parameter vector $\boldsymbol{\eta}$ (a Dirichlet distribution can informally be thought of as a distribution of multinomial distributions \cite{frigyik2010introduction}).

There are $D$ documents in a collection, each with topic proportions $\theta_d$, a multinomial distribution over topics drawn from a Dirichlet distribution with parameter vector $\boldsymbol{\alpha}$ (where $\theta_{d,k}$ is the topic proportion for the $k^{th}$ topic in the $d^{th}$ document).

The $n^{th}$ word in the $d^{th}$ document is assigned topic $z_{d,n}$ (drawn from the multinomial distribution $\theta_d$), with the {\em observed} word $w_{d,n}$ drawn from the multinomial topic distribution for topic $\beta_{z_{d,n}} \, \in \, (\beta_1, .. ,\beta_K$) \cite{blei2003latent}.

The generative model corresponds to the joint distribution of latent and observed variables \cite{blei2011introduction,blei2003latent}:
\begin{align}
P(\beta_{1:K},\theta_{1:D},z_{1:D},w_{1:D}) 
&= \prod_{k=1}^K P(\beta_k|\eta) \prod_{d=1}^D P(\theta_d|\alpha)
\left(\prod_{n=1}^N P(z_{d,n}|\theta_d)P(w_{d,n}|\beta_{1:K},z_{d,n})\right)
\end{align}

To summarise, in the generative model, $K$ topics $\beta_1 .. \beta_K$ are each drawn from $Dir(\boldsymbol{\eta})$ (the Dirichlet distribution with parameter eta). Each document $d$ is then generated by:
\begin{enumerate}
\item drawing $\boldsymbol{\theta}_d \sim Dir(\boldsymbol{\alpha})$
\item for each word $W_{d,n}$ drawing:
\begin{enumerate}
\item topic assignment $Z_{d,n} \sim Mult(\boldsymbol{\theta}_d)$; and
\item generating $W_{d,n} \sim \boldsymbol{\beta}_{Z_{d,n}}$ \cite{blei2011introduction,blei2003latent,steyvers2007probabilistic}.
\end{enumerate}
\end{enumerate}

\subsection{Inferring the latent variables}

The following few paragraphs outline the derivation of the conditional distribution for inferring the latent variables in LDA, as given by Blei \textit{et. al.} (2003) and Steyvers and Griffiths (2007) \cite{blei2003latent,steyvers2007probabilistic}. The full derivation can be found in these papers, particularly, \cite{blei2003latent}. 

The posterior distribution of the latent variables given the observed data and assuming topics $\beta_{1:K}$ are fixed is \cite{blei2011introduction,blei2003latent}: 
\begin{align}
P(\theta_{1:D},z_{1:D}|w_{1:D},\beta_{1:K})
&= \frac{P(\theta|\alpha)\prod_{n=1}^N P(z_n|\theta)P(w_n|z_n,\beta_{1:K})}
{\int\limits_{\theta} P(\theta|\alpha)\prod_{n=1}^N \sum_{z=1}^K P(z_n|\theta)P(w_n|z_n, \beta_{1:K})}
\end{align}

This is intractable to calculate (a ``multiple hypergeometric function" \cite{blei2003latent,dickey1983multiple}). However, the latent topic assignments $z_{d,n}$ can be estimated via Gibbs sampling\footnote{An algorithm for sampling from a multivariate probability distribution by iteratively generating an instance of each variable conditioned on all others.} \cite{geman1984stochastic} on a conditional distribution over a compound Dirichlet-multinomial distribution with the multinomial parameters $\theta_d$ (the per-document topic distributions) and $\beta_k$ (the per-topic distributions over words) integrated out \cite{blei2003latent,steyvers2007probabilistic}:
\begin{align}
P(z_i|z_{-i},w_{1:N}) \propto P(w_i|\beta_{1:K})\prod_{k=1}^K \Gamma(n_k(z_{-i}))
\end{align}
where $n_k(z_{-i})$ is the number of times topic $k$ has been seen in the collection of topic assignments $z_{-i}$ (i.e., all the topic assignments except for assignment $z_i$).

Integrating out the multinomial parameters in the model leaves the observed words $w_{d,n}$, and the $\boldsymbol{\alpha}$ and $\boldsymbol{\eta}$ hyperparameter vectors. These hyperparameters may be inferred or simply set with empirically-derived values \cite{steyvers2007probabilistic}.

The distribution in Equation \ref{eq:gibbs} can be iteratively sampled from to infer each latent topic assignment $z_i$ given the observed words $w_i$ in each document $d_i$ in the collection, and all other topic assignments $z_{-i}$.
\begin{align}
p(z_i=j | z_{-i},w_i,d_i) \propto \frac{C_{w_{ij}}^{WT}+\eta}{\sum_{w=1}^WC_{wj}^{WT}+W\eta}  \frac{C_{d_{ij}}^{DT}+\alpha}{\sum_{k=1}^KC_{d_{i}k}^{DT}+K\alpha}
\label{eq:gibbs}
\end{align}

To perform Gibbs sampling, each word in each document in the collection is initially randomly assigned a topic, and two count matrices are created: $C^{WT}$ of topic assignments to each word in the vocabulary (with $C_{wj}^{WT}$ the number of times topic $j$ is assigned to word $w$ in the collection), and $C^{DT}$ of topic assignments per document (with $C_{d_{i}k}^{DT}$ the number of times topic $k$ is assigned in document $d_i$) \cite{steyvers2007probabilistic} (see Figure \ref{fig:lda-towers}).

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{IMAGES/lda-towers.png}
\caption[Matrices used in LDA]{\textbf{Matrices used in LDA.} Direct counts from the data are grey, inferred ones involving topics are blue, and hyperpriors are yellow. The arrows represent summation if a dimension is being lost, and repeated addition if a dimension is being gained. The $C^{DT}$ and $C^{WT}$ matrices are used in Gibbs sampling to infer the latent topic assignments $z_{d,n}$. The multinomial distributions $\theta_d$ (per-document topic proportions) and $\beta_k$ (distributions of words per topic) can be derived by normalising these matrices respectively \cite{blei2003latent,steyvers2007probabilistic}.}
\label{fig:lda-towers}
\end{figure}

One iteration of Gibbs sampling involves decrementing the matrices at the entry corresponding to each word in the collection in turn, allocating that word a topic from the distribution in Equation \ref{eq:gibbs}, and incrementing the matrices accordingly \cite{steyvers2007probabilistic}. Sampling is run until equilibrium is reached.

The first term in Equation \ref{eq:gibbs}:
\begin{align}
\frac{C_{w_{ij}}^{WT}+\eta}{\sum_{w=1}^WC_{wj}^{WT}+W\eta} \notag
\end{align}
describes the probability of word $w_i$ under topic $j$ (the number of times word $w_i$ is assigned topic $j$ as a proportion of the number of times any word is assigned topic $j$). The second term:
\begin{align}
\frac{C_{d_{ij}}^{DT}+\alpha}{\sum_{k=1}^KC_{d_{i}k}^{DT}+K\alpha} \notag
\end{align}
describes the probability of topic $j$ under the current topic distribution in document $d_i$ (the number of times topic $j$ is found in document $d_i$ as a proportion of all topic assignments in document $d_i$). 

The distributions of words per topic, $\beta$, and topics per document, $\theta$, can be calculated using the first the second terms respectively \cite{steyvers2007probabilistic}.

In essence, LDA uncovers latent topics in a document collection, where words that are likely to co-occur in documents in the collection are found together with high probability within a particular topic or topics (weighted by their overall representation in the document collection).

As an example, a collection of documents might have a vocabulary of words [``ball", ``game", ``win", ``film", ``actor", ``scene"]. The first three words in the vocabulary might be found to occur together in documents with high frequency, but rarely with the last three words (and vice versa). Two topics might be extracted accordingly, a ``sports" topic (under which the first three words are highly likely and the latter three unlikely) and, similarly, a ``movie" topic. A document might be primarily made up of words from one topic, or some mixture of both topics (for example, a review of a sports movie). 

\subsection{Application of LDA to images}

The following analogy can be made between document collections and images: a single greyscale image is a document collection, which comprises $d$ non-overlapping subimages (the documents). The image ``vocabulary" is constructed by taking a histogram of pixel intensities in the entire image, where each of $w$ bins (pixel intensity intervals) is a word in the vocabulary. Any of the binning strategies described in Chapter \ref{C:BIN} can be used to construct the histogram binning. The number of occurrences of a word $w_i$ for document $d_j$ is the count of pixels in subimage $d_j$ that fall into bin $w_i$ of the overall image histogram. Topics are normalised distributions over bins.

Using this model, Gibbs sampling (as described in Section \ref{sec:LDA}) can be run on greyscale images to uncover latent ``topics": distributions of pixel intensities that commonly co-occur in the image, for example a ``background topic".

These topics can then be used to segment the image on a pixel-by-pixel or region-by-region basis. This can be done by assigning a pixel/region a topic based on the most likely topic to have generated the pixel/region. This can be calculated using the probability mass function of the multinomial distribution (Equation \ref{eq:multinom-pdf}, where $x_i$ is the count of pixels in the $i^{th}$ bin, $\sum_{i=1}^kx_i=n$, and $p_i$ is the probability of the $i^{th}$ bin under a particular topic). 
\begin{equation}
\mathrm{Pr}(X_1=x_1,..,X_k=x_k) = \frac{n!}{x_1!..x_k!}p_1^{x_1}..p_k^{x_k} 
\label{eq:multinom-pdf}
\end{equation}

For source detection in radio astronomy images, flood-filling\footnote{Labelling contiguous regions of pixels that have same topic label as a single region.} can then be performed on the segmented image to identify the location and size of sources in the image.

\subsubsection{Related work in image segmentation}

This application of LDA to source-detection in images differs from the approach taken in \cite{cao2007spatially,fei2005bayesian,fergus2005learning,quelhas2005modeling,russell2006using,sivic2005discovering2,sivic2005discovering,sudderth2005learning,wang2007spatial}, in which derivations of LDA and other topic models are applied to image segmentation and object and scene classification tasks. The approach in this chapter relies only on pixel intensity and location, whereas previous approaches employ techniques to extract image interest points and pre-segment images before applying LDA.

A document is a single image in a collection in \cite{fei2005bayesian}, represented by counts of visual words or ``textons" extracted from the image collection.  This model is extended in \cite{cao2007spatially}, in which each image is segmented into homogeneous regions, and each region comprises several interest point patches. Similarly, Russell \emph{et. al.} \cite{russell2006using} compute multiple segmentations on each image in a collection, and compute visual words (interest points) for each candidate segmentation. Interest points are similarly employed in ``bag of words" models in \cite{fergus2005learning,quelhas2005modeling,sivic2005discovering2,sivic2005discovering,sudderth2005learning}. Wang and Grimson \cite{wang2007spatial} added spatial information to their LDA model by grouping visual words close to each other in an image.

In comparison to other flood-filling based astronomical source detection algorithms \cite{masias2012review}, the use of LDA-derived probabilities as a precursor to flood-filling is more powerful than many thresholding algorithms (such as those discussed in \cite{gonzalez2002digital}); in LDA, commonly co-occurring bins needn't be adjacent intensity ranges. Consider an image in which the background is made up of medium-intensity pixels while foreground objects comprise both dark and bright pixels. LDA allows the topics to reflect this, with medium-intensity bins found in one topic, and bright and dark bins in the other.

\begin{table}
\centering
\caption[Astronomical images used for evaluation of LDA]{Astronomical images used for evaluation}
\begin{tabular}{c c c}
\hline
Image & Description & Source\\\hline
A1 & ATLSB\protect\footnotemark survey region A at $50"$ resolution & \cite{saripalli2012atlbs,subrahmanyan2010atlbs}\\
A2 & ATLSB survey region A at $6"$ resolution & \cite{saripalli2012atlbs,subrahmanyan2010atlbs}\\
B1 & ATLSB survey region B at $50"$ resolution & \cite{saripalli2012atlbs,subrahmanyan2010atlbs}\\
B2 & ATLSB survey region B at $6"$ resolution & \cite{saripalli2012atlbs,subrahmanyan2010atlbs}\\
C & ATLAS CDFS\protect\footnotemark & \cite{norris2006deep}\\\hline
\end{tabular}
\label{table:images}
\end{table}
\addtocounter{footnote}{-1}
\footnotetext{Australia Telescope Low Surface Brightness.} 
\addtocounter{footnote}{1}
\footnotetext{Australia Telescope Large Area Survey Chandra Deep Field-South.}

\section{Methods}

LDA was performed for segmentation and source detection in five radio astronomy images (Table \ref{table:images}) Non-astronomical greyscale images were segmented as a demonstration of this application of LDA (see Figures \ref{fig:imseg} and \ref{fig:diff-bins}).

Astronomical images were in FITS format \cite{wells1981fits}; non-astronomical images were greyscale JPEG images.

%Simulated data was created by generating three ``sources" and then overlaying the sources with noise\footnote{This is typically how simulated data is created for evaluation of other Bayesian source detection methods, for examples, see: \cite{carvalho2009fast,feroz2008multimodal,guglielmetti2009background,hobson2003bayesian,savage2007bayesian}.} [EXPLAIN HOW NOISE GENERATED] (see Figure \ref{fig:sim-ims} for examples of simulated images). Sources were generated using a Gaussian ellipse function (with center coordinates $m_x$ and $m_y$, approximate half-widths in orthogonal direction $\sigma_x$ and $\sigma_y$, and rotation parameter $\phi$, and $A$ controlling the height of the peak):
%\begin{align}
%A\exp\left(- \;\left(a(x-m_x)^2+2b(x-m_x)(y-m_y)+c(y-m_y))^2 \right) \right)\label{eq:2d-}
%\end{align}
%with:
%\begin{align}
%a &= \frac{\cos^2(\phi)}{2\sigma_x^2} + \frac{\sin^2(\phi)}{2\sigma_y^2} \label{eq:a-wgt} \\
%b &= \frac{-\sin(2\phi)}{4\sigma_x^2} + \frac{\sin(2\phi)}{4\sigma_y^2} \label{eq:b-wgt} \\
%c &= \frac{\sin^2(\phi)}{2\sigma_x^2} + \frac{\cos^2(\phi)}{2\sigma_y^2} \label{eq:c-wgt}
%\end{align}

For each image a histogram of pixel intensities was generated. An equal width binning strategy was employed with 100 or 1000 bins for astronomical images, and 10 or 100 bins for non-astronomical images. 

Images were decomposed into subimages (``documents"), and counts of pixels in bins were calculated for each. A range of subimage sizes was trialled for each image. 

Gibbs sampling was run to infer per-word topic assignments $z_i$, on the distribution in Equation (\ref{eq:gibbs}), as described in section \ref{sec:LDA}. The $\boldsymbol{\alpha}$ and $\boldsymbol{\eta}$-vectors were set with the empirically derived values $\alpha_i = \alpha_j = 0.1 \; \forall i,j$; $\eta_i = \eta_j = 0.01 \; \forall i,j$ \cite{steyvers2007probabilistic}.

Gibbs sampling was run for 100 iterations. The distributions of words for each topic ($\beta_k$ for $k \in$ \{$\beta_1$ .. $\beta_K$\}) were calculated from the hundredth sample. An average over samples was not taken as it was found that the sampler converged quickly after which the topic distributions changed very little if at all: sample 1000 was virtually identical to sample 500 and sample 100.

To segment the images using the inferred topic distribution, each pixel in the image was assigned the topic that it was most likely generated by using Equation (\ref{eq:multinom-pdf}). When considering a single pixel, this equation simplifies to just $p_i$ for a given bin $i$. That is, if the pixel being considered falls into bin $i$ in the overall pixel intensity histogram, the topic with the greatest probability for bin $i$ is assigned\footnote{Note that this may be weighted by the topic's overall proportion in the collection, however this was not done here.}. 

%\begin{figure}
%\centering
%\includegraphics[width=1.0\textwidth]{IMAGES/sim-ims.png}
%\caption{Simulated astronomical images. Two images with three simulated sources each are shown at left. The images overlaid with noise are shown at right.}
%\label{fig:sim-ims}
%\end{figure}

In the case of the astronomical images, the performance of LDA was compared with the thresholding and region growing astronomical source detection software \emph{Duchamp} \cite{whiting2012duchamp}, on source catalogues generated by manual inspection by an astronomer (a ``ground truth" reference) \cite{saripalli2012atlbs,subrahmanyan2010atlbs}. 

%For simulated astronomical images, performance was evaluated against the actual parameters of the generated sources.

Precision and recall were calculated in order to evaluate the performance of the LDA. Precision is the proportion of true sources of all found sources (that is, all found sources that are really sources):
\begin{align}
\text{precision} &= \frac{\textbf{tp}}{\textbf{tp}+\textbf{fp}}
\end{align}
and recall is the proportion of found sources of all true sources in the image (all true sources that are found):
\begin{align}
\text{recall} &= \frac{\textbf{tp}}{\textbf{tp}+\textbf{fn}}
\end{align}
where $\textbf{tp} =$ ``true positive", $\textbf{fp} =$  ``false positive", and $\textbf{fn} =$ ``false negative"\footnote{Precision and recall are sometimes called ``completeness" and ``reliability" in the astronomical source detection literature \cite{hancock2012compact}.} \cite{olson2008advanced}.

In the case of non-astronomical images, two examples of segmented images are shown for illustrative purposes only (see Figures \ref{fig:imseg} and \ref{fig:diff-bins}).

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{IMAGES/24063-gray.png}
\caption[Image segmented by LDA]{\textbf{Image segmentation.} Each pixel is assigned to the topic it was most likely generated by, as inferred by LDA (for illustrative purposes, each topic is assigned a greyscale value, and each region is coloured according to its topic). Left to right, top to bottom: the original image, followed by the segmented image with two, three, four, five, and six topics. Increasing the number of topics may increase the level of detail revealed by segmentation, but may also introduce spurious segmentations. Original image (top left) from \protect\cite{martin2001database}.}
\label{fig:imseg}
\end{figure}


\section{Results}
\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{IMAGES/diff-bins-1.png}
\caption[The number of bins can affect results]{\textbf{The number of bins in the pixel intensity histogram can affect results.} Top row from left to right: a greyscale JPEG image, the segmented image using 10 bins, and 100 bins. Bottom row from left to right: an astronomical image (with contrast adjusted to see sources), the segmented image using 100 bins, and 1000 bins. Image sources: \protect\cite{karimZebra,saripalli2012atlbs,subrahmanyan2010atlbs}.}
\label{fig:diff-bins}
\end{figure}

Figure \ref{fig:lowresA} demonstrates the segmentation of an astronomical image (Image A1), and the results of flood-filling on the segmented image to identify radio sources in the image.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{IMAGES/lowresA-ims.png}
\caption[Radio astronomy images segmented by LDA]{\textbf{Radio astronomy images segmented by LDA.} From left to right, top to bottom: a radio astronomy image, the image with contrast adjusted and colours inverted in order to see sources, the image segmented using topics derived by LDA (with two topics), the contrast adjusted, colour inverted image overlaid with bounding boxes showing sources identified by flood-filling on the segmented image. Blue borders have been added to delineate the images. Image source: \protect\cite{saripalli2012atlbs,subrahmanyan2010atlbs}.}
\label{fig:lowresA}
\end{figure} 

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{IMAGES/smallray-srcs2.png}
\caption[LDA results on an image polluted with artefacts]{\textbf{LDA results on an image polluted with artefacts.} An astronomical image (top left) with contrast adjusted to see artefacts (top right), seen as concentric circles and radial spikes. LDA (bottom left) falsely identified fewer artefact pixels as sources than \emph{Duchamp} (bottom right). Blue borders have been added to delineate the images.  Image source: \protect\cite{norris2006deep}.}
\label{fig:smallray}
\end{figure}

For the astronomical images, the results of both LDA and \emph{Duchamp} were compared to a source catalogue generated by manual inspection by an astronomer. Although LDA and \emph{Duchamp} perform roughly equivalently with respect to spatially extended, multi-component sources, LDA had more false positives false negatives than \emph{Duchamp} (see Figure \ref{fig:ldavsdu} for an example).

Table \ref{table:LDA-DU} shows the performance of LDA and \emph{Duchamp} as compared to source catalogues generated via manual inspection by an astronomer on sources for which the total flux (intensity) is less than 1.63 mJy\footnote{$1 Jy \equiv 1 \times 10^{-26} W/Hz/M^{2}$}. LDA performed similarly to \emph{Duchamp}. 

LDA sometimes reported a single source where \emph{Duchamp} correctly separated several; however this is due to the post-processing flood-filling, rather than the LDA algorithm itself. In other cases LDA correctly identified sources that \emph{Duchamp} mistakenly merged.

Bright peak pixels seem key to detection by LDA. For example, in image A2 LDA detects several sources below 1.63 mJy, all of which have peak pixels at least $6 \sigma$ above the rms noise\footnote{Noise was manually determined by an astronomer.}; in contrast LDA's false negatives above 1.63 mJy all have peak pixels less than $6 \sigma$ above rms noise.

LDA identified fewer artefact pixels as sources than \emph{Duchamp} in the artefact polluted Image C (Figure \ref{fig:smallray}). This is a clear demonstration of the strength of using a probabilistic model of background. To avoid the effects of such artefacts using \emph{Duchamp} or similar software, an astronomer would need to manually decompose the image into a number of smaller regions and manually adjust region thresholds. Our implementation of LDA avoids such manual interventions.

\begin{table}
\centering
\caption[Comparison of the performance of LDA with \emph{Duchamp}]{Performance of LDA verus \emph{Duchamp}}
\begin{tabular}{c c c c c}
\hline
Image & LDA & &  \emph{Duchamp} & \\
 & Precision & Recall & Precision & Recall \\\hline
A1 & 0.83 & 0.93 & 0.98 & 1.0\\
A2 & 0.98 & 0.99 & 1.0 & 1.0\\
B1 & 1.0 & 0.99 & 1.0 & 0.96\\
B2 & 1.0 & 0.89 & 1.0 & 1.0\\\hline
\end{tabular}
\label{table:LDA-DU}
\end{table}

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{IMAGES/LDA-vs-DuchampV2-3.png}
\caption[A comparison between LDA and \emph{Duchamp}]{\textbf{Performance of LDA (blue) and \emph{Duchamp} \protect\cite{whiting2012duchamp} (red) on four representative regions from Image A2.} Top left: a region containing a radio galaxy with two large jets (and no detected core) seen in projection with other point sources. Both algorithms identify multiple components. Top right: A false positive and a false negative for LDA.  Bottom left: A false positive and a false negative for LDA; \emph{Duchamp} detects three sources less than $2.5 \sigma$ above the rms, missed by LDA (green). Bottom right: A radio galaxy with three components, and a point source. Both algorithms split the radio galaxy into three components. Only \emph{Duchamp} detects the point source less than $2.5 \sigma$ above the rms (green). Image source: \protect\cite{saripalli2012atlbs,subrahmanyan2010atlbs}.}
\label{fig:ldavsdu}
\end{figure}

\section{Discussion}

With regards to the real astronomical images, LDA performed similarly to the standard source-detection software \emph{Duchamp} \cite{whiting2012duchamp} on a representative sample of radio astronomy images, particularly for sources with integrated source flux $2.5 \sigma$ above the rms noise. 

The two algorithms performed similarly with respect to extended, multi-component sources, but LDA had more false positive detections and non-detections than \emph{Duchamp}. 

Bright peak pixels seem to be essential for a source to be detected by the current implementation of LDA. The current implementation of LDA is therefore unlikely to detect any diffuse sources (spatially extended sources with low brightness overall and no bright peak pixels). Possible solutions to this problem are discussed in Section \ref{sec:lda-fw}.

LDA outperformed \emph{Duchamp} on the image polluted with artefacts --- an image that would require labourious manual intervention by an astronomer to detect sources using software such as \emph{Duchamp}. This is a clear demonstration of the utility of the probabilistic model employed.

\subsection{Parameter and computational issues}

In document collections there is a natural segregation of words and documents; LDA was developed for such discrete data \cite{blei2003latent}. However, there is no natural segregation of pixels into intensity ranges or images into regions. In this chapter histogram binning of the image, and decomposition of the image into subimages, was used to discretise the images to resemble document collections. This introduces new parameters to be set. In practice, it was found that the final topics extracted did not vary over a wide range of subimage sizes chosen, however, results did vary based on the number of bins in the histogram (see Figure \ref{fig:diff-bins}). The approach taken to histogram binning in the current chapter is likely to be responsible for this implementation of LDA's poor performance on faint sources in radio astronomy images; this should be addressed in future implementations.

The analogy of pixels to words may be inaccurate for radio astronomy images. The resolution element of a radio telescope is generally several pixels, and so individual pixels are better thought of as having sub-word size (perhaps ``syllables"). The performance of LDA may change if the smallest unit in the model (an individual ``word") is adjusted to take this into account.

The number of topics must be set manually. For source detection two may be sufficient (one each for source and background topics); however for image segmentation in general a different number of topics may give different results. Figure \ref{fig:imseg} shows a grayscale image segmented with two to six topics. With two topics, the object in the image is clearly segmented from the background; increasing the number of topics reveals more details of the image; in general terms increasing the number of topics might be expected to increase the level of detail shown, but may introduce irrelevant detail, for example the segmentation of the sky in Figure \ref{fig:imseg}. For astronomical images, more than two topics could possibly improve results: for example, three topics may result in a background topic and two source topics with one each for bright and dim sources. This may improve performance on diffuse sources without bright peak pixels.

LDA is a ``bag of words" model \cite{blei2011introduction,blei2003latent,steyvers2007probabilistic} and so ignores the natural ordering of pixel intensities. However this may be a benefit, rather than a drawback, as this allows objects made up of non-neighbouring pixel intensity ranges to be correctly segmented from images.

Gibbs sampling to infer the latent topics in LDA is expensive both in terms of computation and time. One Gibbs sample involves iterating through each pixel in the image, allocating each a topic based on the current distributions of words to topics and topics to documents, and so is linear in the number of pixels multiplied by the number of topics. As it is not unusual for astronomical images to be $8000 \times 8000$ pixels, this can be computationally difficult. Additionally, at least one large three dimensional array ($C^{DWT}$ indexing words by documents by topic) must be kept in memory. However, LDA need not necessarily be run for every image, nor on the whole image. LDA could be run on a small representative section of one image in a collection of similar images in order to extract topics for the whole collection. This would reduce the computational expense of the approach.

\subsection{Future work}\label{sec:lda-fw}

The approach described in this chapter shows how LDA can be used for image segmentation and source detection. 

The use of the final topic distributions --- segmentation by assigning each pixel a hard topic label and source detection by flood-filling on the segmented image --- is crude. A more nuanced approach would eliminate this hard assignment and take a more probabilistic approach to region labelling. For example, given the multinomial models for background and source, gradient ascent could be performed to find regions that have high likelihood under a particular model.

Given the reliance on bright peak pixels for source detection by LDA, more work needs to be done to improve LDA's performance on faint sources. In the particular cases analysed, the addition of more bins in the low range of pixel intensities would likely improve performance; in the general case, the optimal use of bins should be investigated.

In many cases, final background and source topics derived by LDA were simple thresholds in bins, with the background distribution over bins placing almost all weight on bins from $0$ to some bin $i$, and the source distribution placing almost all weight from bin $i$ to bin $K$. This is not ideal for discovery of all sources in images in which source and background pixel ranges overlap, and may in fact prevent the discovery of faint sources. In this way, the implementation of LDA in this chapter performs no better than pixel-intensity based thresholding algorithms that restrict search to those sources that contain pixels above a particular threshold. Exploration into why this thresholding in the distribution of pixels happens and how to prevent it is an important avenue for future work.

The implementation of LDA in this chapter does not allow for the use of ``soft" (Dirichlet) bin borders, as described in Chapter \ref{C:BIN}. This is due to the fact that Gibbs sampling is done at the individual pixel (``word") level --- estimating the topic assignment of each pixel conditioned on the topic assignments of all others by:
\begin{itemize}
\item decrementing the $C^{DWT}$ matrix at the given pixel's entry, 
\item estimating a topic assignment for that pixel from the posterior distribution, and 
\item incrementing the matrix accordingly. 
\end{itemize}
In the implementation of LDA in this chapter, individual pixels are described by the bin in which they belong, and topics are distributions over bins. The Dirichlet binning strategies described in Chapter \ref{C:BIN} result in distributions over bins, with partial counts in a number of bins for each pixel: and so each pixel becomes a distribution over words, rather than a single word. Future work should investigate how latent topic assignments can be estimated given distributions over words, rather than discrete counts of words. 

With regards to the additional application of LDA in segmenting non-astronomical greyscale images, the performance of this could be assessed by comparing the obtained segmentation with human segmentation of the same images, using a large public database of images such as \cite{martin2001database}. This would also allow comparison to the results obtained by other image segmentation algorithms. 

\section{Conclusions}

This chapter presents a preliminary investigation into use of the topic model latent Dirichlet allocation for image segmentation in greyscale images and source detection in astronomical images. The method builds a probabilistic model of ``non-source" pixel distributions.

LDA performed similarly to the standard source-detection software \emph{Duchamp} \protect\cite{whiting2012duchamp} on a representative sample of radio astronomy images, however, for fainter sources and in particular diffuse sources, there is still some work to be done to explore if the LDA method will be an improvement over existing algorithms.

A particular success of the approach is the superior result obtained in Image C, which is polluted with artefacts, as compared to the relatively poor performance of \emph{Duchamp}.

The algorithm could be refined to take a more probabilistic approach to region labelling rather than the hard assignment described in the current chapter, along with further exploration of the optimal pixel binning strategy.




