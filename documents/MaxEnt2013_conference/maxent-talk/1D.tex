\chapter{A Dirichlet-multinomial score: derivation}\label{C:1D}

Given an astronomical image and a model for background, it is possible to assign a score to a particular region of that image on the basis of how well the region conforms to the model. If, in addition to the model for background, there is a model for foreground, a score can be constructed to indicate which model best fits the region.

This chapter describes how a Dirichlet-multinomial distribution may be used to calculate such a score on discretised images.

To find peaks in score-space --- that is, regions which poorly conform to a background model, and/or regions which conform well to a foreground model --- one approach is to exhaustively calculate the score for all possible regions of the data. This is inefficient and time-consuming, particularly as the size of the data and the number of parameters defining a region grows. Alternatively, the gradient of the score may be calculated, and gradient-ascent performed to find peaks in the data.

To illustrate the development of the score, one dimensional data is used as well as two dimensional data. In the context of two-dimensional astronomical images, one-dimensional data is a $1 \times X$ ``slice" through the image at some $y$-position; each pixel location $x \in [0..X] $ has an intensity value $\in \mathbb{R}$. The methods described for one dimensional data are extended to two-dimensional data in each section of this chapter.

Two parameters are used to describe regions in one-dimensional data: an $x$-position $m$ and an approximate half-width $\sigma$, which are denoted collectively as $\theta$.

Gaussian elliptical regions are used for two-dimensional data, where the region's parameters $\theta$ are center coordinates $m_x$ and $m_y$, approximate half-widths in orthogonal direction $\sigma_x$ and $\sigma_y$, and rotation parameter $\phi$. 

The gradient of the Dirichlet-multinomial score (DM-Score) with respect to $\theta$ for one and two dimensional data is derived. 

Similarities and differences between the Dirichlet-multinomial score and latent Dirichlet allocation (Chapter \ref{C:LDA}) are discussed in Section \ref{sec:comp-lda}.

In Chapter \ref{C:2D} the results of source detection via gradient ascent on the Dirichlet-multinomial score are presented.

\section{Notation}

\subsection{Representing an image in terms of binned values}

Each pixel in an astronomical image has an intensity value $\in \mathbb{R}$; these values can be discretised by ``binning" them under some predefined scheme, as described in Chapter \ref{C:BIN}. Within any given region in an image, a histogram of counts \textbf{n} in $K$ bins can be formed. 

Wherever possible $x$ is used to refer to a pixel in an image in the case of one dimensional data, and $k$ to refer to a bin index.  The raw but binned image is denoted by $\mathbf{b}$, and so $b_x$ is the index of the bin that results from the pixel intensity at point $x$ in the image.

It will be useful to define the variable $C^x_k =\delta_{b_x,k}$ where
$\delta_{i,j}=1$ {\it iff} \, $i=j$, and is zero otherwise. Here $C$
is an indicator variable: in effect, simply a verbose way of
representing the integer $b_x$ as an entire vector consisting of a single 1 (corresponding to $b_k$, the index of the bin $k$ that contains pixel $x$) and $K-1$ many 0's. This notation appears cumbersome, but it is introduced in order to be able to extend it to normalised vectors, thus allowing $C$ to represent ``partial counts'' across more than one bin\footnote{Specifically, it is introduced to facilitate the use of Dirichlet borders (a novel way to derive partial counts as a direct consequence of the uncertainty about where bin borders should be placed, as described in Section \ref{sec:dirichlet-borders} in Chapter \ref{C:BIN}).}.

The aggregated bin counts for some region $\mathbf{R}$ can then be written:
\begin{align}
n_k^{(\theta)} &= \sum_{x \in R} C^x_k \label{eq:def-cxk}
\end{align}

The variable $C^x_k =\delta_{b_x,k}$ described in Equation \ref{eq:def-cxk} in may be extended to $C^{x,y}_k =\delta_{b_{(x,y)},k}$ for two-dimensional data by denoting the aggregated bin counts for some region $\mathbf{R}$:
\begin{align}
n_k^{(\theta)} &= \sum_{(x,y) \in R} C^{x,y}_k
\end{align}
where $\delta_{i_{x,y},j}=1$ {\it iff} \, $i=j$ (that is, if pixel $i$ at position $(x,y)$ falls into bin $j$), and is zero otherwise.

\subsubsection{Definition of a hard region: one dimensional data}

A hard region has hard (``all or nothing'') borders, which can be thought of as a rectangular bounding-box around some region of the data (see Figure \ref{fig:hard-soft-regions}). For such a region, $\mathbf{R}$ is defined as:
\begin{align}
\mathbf{R}^{(\theta)} \in [x_{m-\sigma},x_{m+\sigma}] \label{eq:hard-borders}
\end{align}

Where the parameters $\theta$ are $m$ denoting $x$-position and $\sigma$ denoting half-width.

A hard region can also be described by a weighting function $W^{(\theta)}_x$ over points $x$ in the region:
\begin{align}
W^{(\theta)}_x =\begin{cases}  
1, & \quad \text{if $x \in \mathbf{R}$}.\\
0, & \quad \text{otherwise}.
\end{cases}
\label{eq:hard-borders-wgt}
\end{align}

Alternatively, a graded weighting may be applied to a region to ``soften" the borders of that region.

\subsubsection{Definition of a soft region: one dimensional data}
A region with ``soft'' borders can similarly be defined by a weighting function $W^{(\theta)}_x$. The function's parameters $\theta$ could, for example, involve a center point $m$ and approximate half-width $\sigma$, as in the Gaussian function \cite{wasserman2004all}:
\begin{align}
W^{(\theta)}_x 
&= \exp\left( -\frac{(x-m)^2}{2 \sigma^2} \right) \label{eq:region-weight-in-1d} 
\end{align}

Such a region with ``soft" borders can be thought of as a weighted curve across the image, rather than a rectangular bounding box, as depicted in Figure \ref{fig:hard-soft-regions}. Soft bordered test regions may be expected to better reflect the nature of astronomical data: astronomical objects do not have hard borders but rather their intensity fades toward their edges.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{IMAGES/hard-soft-borders.png}
\caption[Two methods for defining a region]{\textbf{Two methods for defining a region of data} with $m = 30$ and $\sigma=10$. The pink rectangular region illustrates a ``hard-bordered" region as defined in Equation \ref{eq:hard-borders}: each pixel within the pink region contributes a count of 1 to its corresponding bin. The region under the blue curve illustrates a ``soft-bordered" region as defined in Equation \ref{eq:region-weight-in-1d}: a pixel at position $x$ contributes a count corresponding with the blue curve's position on the $y$-axis: for example, a pixel at position $20$ on the $x$-axis increments the count in bin $j$ by approximately $0.4$.}
\label{fig:hard-soft-regions}
\end{figure}

\subsubsection{Definition of a soft region: two dimensional data}

For two dimensional data, a region with ``soft'' borders can be defined by a
weighting function $W^{\theta}_{x,y}$ over points $(x,y)$, with
parameters $\theta$ specifying the position and shape of the region.

For example, to specify a Gaussian elliptical region, the function's
parameters $\theta$ could involve center coordinates $m_x$ and $m_y$, approximate half-widths in orthogonal direction $\sigma_x$ and
$\sigma_y$, and rotation parameter $\phi$. 

Without the rotation parameter, the weighting function could be defined as in \cite{rencher2003methods}:
\begin{align}
W^{\theta}_{x,y}
&= e^f, \;\;
\text{   with   } \;\;
f = -\frac{\Delta_x^2}{2\sigma_x^2} - \frac{\Delta_y^2}{2\sigma_y^2} \label{eq:region-weight-in-2d-nophi}  
\intertext{where $\Delta$ signifies a displacement from the central position,}
\Delta_x &= x-m_x, \;\;\;\;\text{and} \;\;\; \Delta_y = y-m_y  \label{eq:Delta-defn}  
\end{align}

This elliptical region can be rotated by applying a rotation matrix (incorporating the rotation parameter $\phi$ into $\Delta_x$ and $\Delta_y$) \cite{rencher2003methods}:
\begin{align}
\begin{bmatrix}
\Delta_x^{'}\\
\Delta_y^{'}
\end{bmatrix}
=
\begin{bmatrix}
\cos(\phi) & -\sin(\phi)\\
\sin(\phi) & \cos(\phi)
\end{bmatrix}
\begin{bmatrix}
\Delta_x\\
\Delta_y
\end{bmatrix}
=
\begin{bmatrix}
\Delta_x \cos(\phi) - \Delta_y \sin(\phi)\\
\Delta_x \sin(\phi) + \Delta_y \cos(\phi)\\
\end{bmatrix} \label{eq:rotation-matrix}
\end{align}

The weighting function with the rotation matrix applied is therefore:
\begin{align}
W^{\theta}_{x,y} 
&= \exp\left(-\frac{\Delta_x^{'2}}{2\sigma_x^2} - \frac{\Delta_y^{'2}}{2\sigma_y^2}\right) \label{eq:region-weight-in-2d}
\end{align}
Expanding the terms $\Delta_x^{'2}$ and $\Delta_y^{'2}$ gives:
\begin{align}
\Delta_x^{'2}
&= (\Delta_x\cos(\phi)-\Delta_y\sin(\phi))^2 \\
&= \Delta_x^2 \cos^2(\phi) -2(\Delta_x\cos(\phi)\Delta_y\sin(\phi)) + \Delta_y^2\sin^2(\phi)\label{eq:delta_x_dash}
\end{align}
and:
\begin{align} 
\Delta_x^{'2}
&= (\Delta_x\sin(\phi)+\Delta_y\cos(\phi))^2 \\
&= \Delta_x^2 \sin^2(\phi) + 2(\Delta_x\sin(\phi)\Delta_y\cos(\phi)) + \Delta_y^2\cos^2(\phi)\label{eq:delta_y_dash}
\end{align}
Putting equations \ref{eq:delta_x_dash} and \ref{eq:delta_y_dash} back into \ref{eq:region-weight-in-2d} and simplifying yields:
\begin{align}
f &= \exp\left(- \;\left(a\Delta_x^2+2b\Delta_x\Delta_y+c\Delta_y^2 \right) \right)\label{eq:region-weight-in-2d-simpl}
\intertext{with:}
a &= \frac{\cos^2(\phi)}{2\sigma_x^2} + \frac{\sin^2(\phi)}{2\sigma_y^2} \label{eq:a-wgt} \\
b &= \frac{-\sin(2\phi)}{4\sigma_x^2} + \frac{\sin(2\phi)}{4\sigma_y^2} \label{eq:b-wgt} \\
c &= \frac{\sin^2(\phi)}{2\sigma_x^2} + \frac{\cos^2(\phi)}{2\sigma_y^2} \label{eq:c-wgt}
\end{align}

\subsubsection{Bin counts in a region}

For one dimensional data, the aggregated bin counts in a region (using any of the histogram binning strategies described in Chapter \ref{C:BIN}) can be multiplied by the weighting function (such as those given in Equations \ref{eq:hard-borders-wgt} and \ref{eq:region-weight-in-1d}) to give ``weighted counts'', $\hat{C}$:
\begin{align}
\hat{C}^x_k &= C^x_k  \; W^{\theta}_x
\end{align}
Similarly, for two dimensional data:
\begin{align}
\hat{C}^{x,y}_k &= C^{x,y}_k  \; W^{\theta}_{x,y}
\end{align}
(Note: to avoid clutter, $\hat{C}$'s dependence on $\theta$ is omitted in this notation).

The aggregated bin counts for the region defined by $\theta$ in one dimensional data can then be denoted:
\begin{align}
n_k^{(\theta)} 
&= \sum_x \; \hat{C}^x_k   
\label{eq:soft-bin-counts-1d} 
\end{align}
and similarly for two dimensional data:
\begin{align}
n_k^{(\theta)} 
&= \sum_{x,y} \; \hat{C}^{x,y}_k  \label{eq:soft-bin-counts-2d} 
\end{align}

Note that if a region is defined with ``soft'' borders as in Equation \ref{eq:region-weight-in-1d} the sum is extended to potentially all pixels in the image. However in practice, for one dimensional data, the sum over $x$ can be truncated to a local mask for which $W^{(\theta)}_x \geq \epsilon$, for some suitably small value of $\epsilon$, or alternatively, by applying the weighting function to the data as in Equation \ref{eq:soft-bin-counts-1d} and truncating the resulting vector to the region defined by $\mathbf{R}^{(\theta)} \in [x_{m-a\sigma},x_{m+a\sigma}]$ for some value of $a$.

For two dimensional data, the Equation in \ref{eq:soft-bin-counts-2d} could be similarly truncated to a local mask or by applying Equation \ref{eq:soft-bin-counts-2d} and truncating the resulting array, for example to a bounding box centered on $(m_x,m_y)$.

\section{Scoring a region}

\subsection{The Dirichlet-multinomial distribution}

The Dirichlet-multinomial distribution is a compound probability distribution, where the parameter vector $\textbf{p} = p_1, ..., p_K$ of a multinomial distribution (with the probability that value $k$ is drawn from $\textbf{p}$ given by $p_k$), is drawn from a Dirichlet distribution with parameter vector $\boldsymbol\alpha = \alpha_1, ..., \alpha_K$ \cite{kotz2004continuous,ng2011dirichlet}.

For some vector of counts $\textbf{n}^{(\theta)}$ in $K$ bins of a histogram (where the counts are taken within a region defined by $W^{(\theta)}$), integrating out the multinomial distribution gives the following marginal joint likelihood in terms of hyperparameter $\boldsymbol{\alpha}$ \cite{kotz2004continuous,ng2011dirichlet}:

\begin{align}
P(\textbf{n}^{(\theta)}|\boldsymbol{\alpha}) &= \frac{\Gamma(A)}{\Gamma(N+A)} \prod_k \frac{\Gamma(n_k+\alpha_k)}{\Gamma(\alpha_k)}  \label{eq:muldir} 
\intertext{where}
A &= \sum_k \alpha_k \\
N &= \sum_k n_k
\end{align}

The logarithm of this is: 
\begin{align}
\log P(\textbf{n}^{(\theta)}|\boldsymbol{\alpha}) &= \log \Gamma(A) - \log \Gamma(N+A) + \sum_k \log \Gamma(n_k+\alpha_k) - \log \Gamma(\alpha_k) \label{eq:logmultdir}
\end{align}

\subsection{A Dirichlet-multinomial score}\label{sec:dir-score}
A good ``score" should give an indication of how poorly a histogram of pixel intensities in a particular region (that is, binned counts) conforms to a model of background, where the model is histograms that might be expected in regions of pure ``background'' signal. (Or alternatively, how poorly a region's histogram conforms to a model of background as compared to how well it conforms to a model of foreground).

Such a score can be calculated from the ratio of posterior probabilities under the two models\footnote{This is a form of a Bayes factor, which is a Bayesian model selection method for data $D$ and $M_1$ and $M_2$ the two competing models, with $\frac{P(D|M_1)}{P(D|M_2)}$ \cite{kass1995bayes}. Evaluating terms involves integrating over the parameters of the model. For the Dirichlet-multinomial score, conveniently the integral is analytic (Equation \ref{eq:muldir}).}:

\begin{align}
\text{Score} &= \log \frac{P(S | \textbf{n}^{(\theta)})}{P(B | \textbf{n}^{(\theta)})} \label{eq:score} \\
&= \log \frac{P(\textbf{n}^{(\theta)} | S)}{P(\textbf{n}^{(\theta)} | B)} \;\; + \; \log \frac{P(S)}{P(B)}\label{eq:score2}
\end{align}

where $S$ is the Dirichlet-multinomial distribution with hyperparameter vector $\boldsymbol{\alpha}^S$ and similarly for $B$ with $\boldsymbol{\alpha}^B$.

If it is assumed that there is an equal mixture of source and background pixels in an image, the second term in Equation \ref{eq:score2} is equal to zero and may be dropped. This may be a fair assumption if nothing is known about the actual mixture of background and source in images or regions within an image. However, because it is known that background pixels far outweigh source pixels in radio astronomy images, this information can be incorporated into the score.

For the gradient calculation, the second term is a constant and so may be dropped, leaving:

\begin{align}
\log \frac{P(\textbf{n}^{(\theta)} | S)}{P(\textbf{n}^{(\theta)} | B)}
&= \log {P(\textbf{n}^{(\theta)} | S)} - \log {P(\textbf{n}^{(\theta)} | B)} \label{eq:score3}
\end{align}

Writing out Equation \ref{eq:score3} in full gives:
\begin{align}
\log {P(\textbf{n}^{(\theta)} | S)} - \log {P(\textbf{n}^{(\theta)} | B)} &= \sum_k \log \Gamma (n_k + \alpha^S_k) - \log \Gamma (N + A^S) \notag\\
&- \sum_k \log \Gamma (n_k + \alpha^B_k) + \log \Gamma (N + A^B) \label{eq:dirmult-score}
\end{align}

Note that no reparameterisation of Equation \ref{eq:dirmult-score} is necessary for extension to two-dimensional data, as $\textbf{n}^{\theta}$, $\boldsymbol{\alpha}^S$, and $\boldsymbol{\alpha}^B$ are all $K$-length vectors (where $K$ is the number of bins in a histogram binning of the data).


\section{The gradient of the score}

It is possible to find peaks in the score by exhaustively iterating through all combinations of values of $\theta$ and calculating the score for each. However this is obviously time consuming, and potentially intractable with increasing sizes of images and number of parameters $\theta$. A more efficient approach is to perform gradient ascent to find maxima.

Taking the derivative of the score (equation
\ref{eq:dirmult-score}) with respect to region parameters $\theta$ (denoting the derivative of the log of the $\Gamma$ function by $\psi$), yields:

\begin{align}
\frac{\partial}{\partial\theta}\text{Score}(\theta) 
&= \sum_k [\underbrace{\psi(n_k + \alpha^S_k) - \psi(n_k + \alpha^B_k)}_{\text{denote} \; Q_k}] \frac{\partial n_k^{(\theta)}}{\partial\theta} \notag\\
& - \;\;\; [\psi(N+A^S) - \psi(N+A^B)]\sum_k \frac{\partial n_k^{(\theta)}}{\partial\theta}\\
&= \sum_k  Q_k \, \frac{\partial n_k^{(\theta)}}{\partial\theta}\;-\;
\sum_k
\underbrace{ [\psi(N+A^S) - \psi(N+A^B)}_{\text{denote} \; Q_\text{base}}] \frac{\partial n_k^{(\theta)}}{\partial\theta}\\
&= \sum_k (Q_k - Q_\text{base}) \; \frac{\partial n_k^{(\theta)}}{\partial\theta} \label{eq:score-grad-1d}
\end{align}
with the above definitions for $Q_k$ and $Q_\text{base}$.

The remaining gradient term can then be calculated from Equation \ref{eq:soft-bin-counts-1d} for one dimensional data:
\begin{align}
\frac{\partial n_k^{\theta}}{\partial\theta} &= \sum_{x} \, C^{x}_k \; \frac{\partial W^{\theta}_{x}}{\partial\theta} \\
&= \sum_{x} \, C^{x}_k \;  W^{\theta}_{x} \; \frac{\partial f}{\partial\theta} \\
&= \sum_{x} \;\; \hat{C}^{x}_k \;\; \frac{\partial f}{\partial\theta}\label{weight-grad-1d}
\end{align}
where $f = \; \log W^{\theta}_x$

And similarly for two dimensional data from Equation \ref{eq:soft-bin-counts-2d}:
\begin{align}
\frac{\partial n_k^{\theta}}{\partial\theta} &= \sum_{x,y} \, C^{x,y}_k \; \frac{\partial W^{\theta}_{x,y}}{\partial\theta} \\
&= \sum_{x,y} \, C^{x,y}_k \;  W^{\theta}_{x,y} \; \frac{\partial f}{\partial\theta} \\
&= \sum_{x,y} \;\; \hat{C}^{x,y}_k \;\; \frac{\partial f}{\partial\theta}
\label{weight-grad-2d}
\end{align}
where $f = \; \log W^{\theta}_{x,y}$

Putting these equations back together, in general the gradient of the score for one dimensional data is:
\begin{align}
\frac{\partial}{\partial\theta}\text{Score}(\theta) 
&= \sum_k (Q_k - Q_\text{base}) \; \sum_{x} \hat{C}^{x}_k \;\; \frac{\partial f}{\partial\theta}
\label{eq:general-gradient-1d}
\end{align}
and similarly for two dimensional data:
\begin{align}
\frac{\partial}{\partial\theta}\text{Score}(\theta) 
&= \sum_k (Q_k - Q_\text{base}) \; \sum_{x,y} \hat{C}^{x,y}_k \;\; \frac{\partial f}{\partial\theta}
\label{eq:general-gradient}
\end{align}


Notice the data are involved in (up to) four places:
\begin{itemize}
\item $Q_k$, which depends on $n_k$, the $k^{th}$ bin in $\textbf{n}^{(\theta)}$, the aggregated weighted counts in $K$ bins in the region defined by $\theta$,
\item$Q_\text{base}$, which depends on $N=\sum_k{n_k}$ (note that this does not vary with the position of the region, except for edge effects at the edges of the image, but does vary with the region's size),
\item $\hat{C}^x_k$ or $\hat{C}^{x,y}_k$; some simplification is possible in the case that this is a strict ``indicator function'' (zero everywhere except one bin), but not in general, and
\item $\frac{\partial f}{\partial\theta}$
\end{itemize}
but always via the weighted counts $\hat{C}$. 

\subsection{The gradient, given a specific parameterisation for $W$}\label{sec:grad-score}

Any window function $W$ may be used in Equation \ref{eq:general-gradient-1d} (one dimensional) or \ref{eq:general-gradient} (two dimensional) to arrive at the full score and its gradient calculation. 

The gradients calculated can be substituted into Equation \ref{eq:general-gradient-1d} or \ref{eq:general-gradient} giving the full gradient, which can then be provided to any gradient-based optimization routine.

\subsubsection{One dimensional data}

Using the function for a soft window given in Equation \ref{eq:region-weight-in-1d} as an example, whose parameters $\theta$ are $(m, \sigma)$, the gradient is as follows.

The derivative with
respect to $m$ (the $x$-position of the mid-point of the window) is:
\begin{align}
\frac{\partial f}{\partial m} \; &= \; W^{(\theta)}_x \frac{(x-m)}{\sigma^2}
\intertext{and so}
\frac{\partial}{\partial m}\text{Score}(\theta) \;
&= \; \sum_k (Q_k - Q_\text{base}) \; \sum_x \, \hat{C}^{x}_k \; \frac{(x-m)}{\sigma^2} \label{eq:1dgrad-wrt-position}
\end{align}

With respect to $\sigma$, it is

\begin{align}
\frac{\partial f}{\partial \sigma} &= W^{(\theta)}_x \frac{(x-m)^2}{\sigma^2} \frac{1}{\sigma}
\intertext{and so}
\frac{\partial}{\partial\sigma}\text{Score}(\theta) \;
&= \; \sum_k (Q_k - Q_\text{base}) \; \sum_x \, \hat{C}^{x}_k \frac{(x-m)^2}{\sigma^3} \label{eq:1dgrad-wrt-sigma}
\end{align}

These two numbers can be substituted into Equation \ref{eq:general-gradient-1d} to calculate the gradient of the score with respect to the complete parameter set.


\subsubsection{Two dimensional data}

Similarly for two dimensional data, using the function for a two-dimensional ellipse given in Equation \ref{eq:region-weight-in-2d-simpl} as an example, whose parameters $\theta$ are $(m_x, m_y, \sigma_x, \sigma_y, \phi)$ the gradient can be calculated as follows.

The derivative with respect to $m_x$ is:
\begin{align}
\frac{\partial}{\partial m_x}\text{Score}(\theta) \;
&= \; \sum_k (Q_k - Q_\text{base}) \; \sum_{x,y} \, \hat{C}^{x,y}_k \;\; \frac{\partial f}{\partial m_x}
\intertext{differentiating Equation \ref{eq:region-weight-in-2d-simpl}, with the definition for $\Delta_x$ and $\Delta_y$ as per Equation \ref{eq:Delta-defn}, gives}
\frac{\partial f}{\partial m_x} \; &= \; 2a\Delta_x  + 2b\Delta_y 
\intertext{so the gradient calculation for this parameter is:}
\frac{\partial}{\partial m_x}\text{Score}(\theta) \;
&= \; \sum_k (Q_k - Q_\text{base}) \; \sum_{x,y} \, \hat{C}^{x,y}_k \; \bigg[ \, 2a\Delta_x + 2b\Delta_y \bigg] \label{eq:2dgrad-wrt-delta-x}
\end{align}

More generally, differentiating equation
\ref{eq:region-weight-in-2d-simpl} with respect to each of the five
parameters $m_x, m_y, \sigma_x, \sigma_y$ and $\phi$, gives the
following:
\begin{align}
\frac{\partial f}{\partial m_x} \; &= \; 2a\Delta_x + 2b\Delta_x \\ \notag\\ 
\frac{\partial f}{\partial m_y} \; &= \; 2b\Delta_x + 2c\Delta_y \\ \notag\\ 
\frac{\partial f}{\partial \sigma_x} \; &= \;
\frac{1}{\sigma_x^3} \; \bigg( \Delta_x^2\cos^2(\phi) -
\Delta_x\Delta_y\sin(2\phi) + \Delta_y^2\sin^2(\phi) \bigg)\\ \notag\\ 
\frac{\partial f}{\partial \sigma_y} \; &= \;
\frac{1}{\sigma_y^3} \; \bigg( \Delta_x^2\sin^2(\phi) +
\Delta_x\Delta_y\sin(2\phi) + \Delta_y^2\cos^2(\phi) \bigg) \\ \notag\\ 
\frac{\partial f}{\partial \phi} \; &= \; \;
\frac{1}{2} \,\bigg( \frac{1}{\sigma_x^2} - \frac{1}{\sigma_y^2}
 \bigg) \;
\bigg(  
(\Delta_x^2 - \Delta_y^2) \, \sin(2\phi) \; + \; 2 \Delta_x \Delta_y \cos(2\phi)
 \bigg)
\end{align}

\section{Comparison with latent Dirichlet allocation}\label{sec:comp-lda}
There are many similarities between latent Dirichlet allocation (as used in Chapter \ref{C:LDA}) and the Dirichlet-multinomial score.

Both techniques use multinomial distributions over bins as models for source and background; and in both methods these distributions are part of a compound Dirichlet-multinomial distribution with the multinomial parameter integrated out, leaving only the data and the Dirichlet hyperparameter(s).

However, LDA estimates a conditional distribution while the DM-Score calculates a marginal joint distribution \cite{blei2003latent, ng2011dirichlet}. In addition, the multinomial models for source and background are easily derived from the LDA process, but remain latent in the DM-Score. 

These differences are relatively minor; however there are two differences between the models that are more fundamental. In LDA, there is one $\boldsymbol{\alpha}$ hyperparameter over all topic distributions $\beta_k \in K$ (where $K=2$ for source and background in radio astronomy images). In the case of the DM-Score, there are two $\boldsymbol{\alpha}$ hyperparameters: one each for source and background distributions. Under LDA's generative model the multinomial distributions over bins for source and background are fixed across the whole image; while under the generative model for the DM-Score, they may vary between sub-images.

With regards to these two points, there is a conceptual similarity between the DM-Score described in this chapter and the DCMLDA model described in \cite{doyle2009accounting,elkan2006clustering,madsen2005modeling} (DCMLDA is: Dirichlet compound multinomial latent Dirichlet allocation).

DCMLDA is largely the same as LDA as described in Chapter \ref{C:LDA}, except for one key difference: LDA has one multinomial distribution $\beta_k$ for each of $K$ topics, all of which have the same Dirichlet hyperparameter $\eta$. In the case of DCMLDA however, each topic $k$ has its own Dirichlet hyperparameter $\eta_k$, from which a multinomial topic distribution over words $\beta_{k,d}$ is drawn for each topic $k$ and each document $d$ in the document collection \cite{doyle2009accounting}.

The DCMLDA model was developed to model the phenomenon of ``burstiness", a term which describes the behaviour of words appearing in ``bursts" --- that is, if a word appears once in a document, it is more likely to appear again in that same document, even if that word is relatively rare in the corpus and does not appear in most documents. This phenomenon is not well modelled by LDA, where a document is a mixture of topics, and each topic describes how likely each word in the vocabulary is to appear under that topic. Under the LDA model, each word in a topic has a particular probability of occurring, which is static from document to document \cite{doyle2009accounting,elkan2006clustering,madsen2005modeling}.

Doyle and Elkan \cite{doyle2009accounting} give the following example: given a ``sports" topic in a document collection, in which the words ``rugby" and ``hockey" have equal probability of occurring, the LDA model will give both these words equal probability of occurring within a document. However, this is a poor model given that
one occurrence of the word ``rugby" in a document means a subsequent occurrence of the word ``rugby" is more likely than a first occurrence of the word ``hockey".

In the context of the DM-Score for radio astronomy images, this allows the variability of source and background distributions, particularly over an image, to be accounted for. For example, the variability of an image's background distribution, background regions at the edge of radio astronomy images are more noisy than those at the centre of an image (see Figure \ref{fig:background-var} for an illustration of this point).

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{IMAGES/background-var.png}
\caption[Variation in background across an image]{\textbf{Background variation.} Two radio astronomy images with contrast adjusted to show the variation in background across an image. Note the greater noise at the edges of the images. Images: Chandra Deep Field South (CDFS; top) and European Large Area ISO Survey S1 (ELAIS; bottom) \cite{norris2006deep}}
\label{fig:background-var}
\end{figure}


