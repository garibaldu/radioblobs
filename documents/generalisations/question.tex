\documentclass[12pt]{article}
\input{LocalStyle.sty}  
% I keep MarcusStyle.sty in $HOME/texmf/tex/latex, because 
% kpsewhich -var-value=TEXMFHOME says $HOME/texmf is where latex looks.

%\setlength{\parskip}{2mm plus 1mm minus 1mm}
\renewcommand{\arraystretch}{1.5}
\renewcommand*{\familydefault}{\sfdefault}

\title{generalising and extending our use of DirMult for source finding}
\author{marcus}
\date{}
\begin{document}
\maketitle

\tableofcontents

\section{The big picture - the way we approach source-finding}

Goal: automatically find all the sources in an astronomical image, in
a way that's robust and fast.  In most of the the discussion to
follow, we will assume there is a single source for
simplicity. Generalising the approach to multiple sources will be
addressed..... later!

Basic approach: We will consider two models denoted $B$ for {\bf
  b}ackground and $C$ for sour{\bf c}e.  One can derive a score for
how well these locations match regions of ``non-background'' pixel
intensities (and potentially other features), and treat source-finding
as an exercise in optimizing those parameters.

For $k$ categories (bins) we will have a $k$-elt vector $\bn$ of
``counts'', for each class.  We will use a superscript to denote the
class, with $B$ for background and $S$ for source.

Figure \ref{fig:schematic} shows the structure of this generative
model for images schematically. The image is modelled as consisting of
a source region within the image, and ``counts'' of features inside
($\bn_C$) and outside ($\bn_B$) that region. The features being
counted could be as simple as the raw pixel intensities for example,
appropriately discretized (binned).


\begin{figure}
\begin{center}
\includegraphics[scale=1.0]{./pics/FreFriPGM_Schematic} 
\end{center}
\caption{Schematic structure of a probabilistic graphical model (PGM)
  for the counts $n_B$ observed in the ``background'' region of an
  image and the counts $n_C$ arising from an astronomical source affecting the image.  \label{fig:FreFriPGM_Schematic} }
\end{figure}


Two big questions then: what score? and  what model?


\section{what score?}

We will denote the regions specifying the ``source region'' (or
perhaps more accurately, the boundary between the source and
background regions) by $\theta$. How should we ``score'' how well a
given choice for $\theta$ identifies the extent of a source in the
image?

\subsection{proposal 1: Bayes factor}

One possible score is ratio of the likelihood of the counts inside the putative source region under the two models. The log of this is
\begin{align}
S_\text{likRatio} &= \log \bigg[ \frac{\Pr(\bn^S \mid S, \theta)}{\Pr(\bn^S \mid B, \theta)} \bigg]
\intertext{A criticism of the above is that it takes no account of how the background probabilities of source and background. A 
a better variant might be to use the ratio of {\it posterior} probabilities instead, often called a "Bayes factor":}
S_\text{BayesFactor} &= \log \bigg[ \frac{\Pr(S \mid \bn^S, \theta )}{\Pr(B \mid \bn^S, \theta)} \bigg] \nonumber\\
%&= S_\text{likRatio}  \;\; + \log \bigg[ \frac{\Pr(S )}{\Pr(B)} \bigg] \nonumber\\
&= \log \big[ \Pr(\bn^S \mid S, \theta) \big] \;\; - \log \big[\Pr(\bn^S \mid B, \theta) \big]  \;\; + \text{Offset}
\label{eq:BayesFactor}
\end{align}
The $ \text{Offset}$ is $\log [\Pr(S ) / \Pr(B)] $ and is a constant
reflecting how much of a {\it typical} image we expect will be due to
the effects of actual sources. As it is not a function of the
particular choice of source region being optimized, it will not play a
role in that optimization. However it does enable us to give meaning
to the value of $S_\text{BayesFactor}$: if this is positive, we prefer
``source'' over ``background'' as an explanation for the counts within
that region.

\subsection{proposal 2: likelihood}

An alternative score is log likelihood of the whole image under the combined model, under the current choice of region:
\begin{align*}
S_\text{likelihood} 
&= \log \big[ \Pr(\bn^S, \bn^B \mid S, B, \theta) \big]\\
&= \log \big[ \Pr(\bn^B \mid B, \theta) \;\; \Pr(\bn^S, \mid S, \theta) \big] \\
&= \log \big[ \Pr(\bn^B \mid B, \theta) \big] \;\; + \;\; \log \big[ \Pr(\bn^S \mid S, \theta) \big]
\end{align*}

How does this relate to  $S_\text{BayesFactor}$ in equation \ref{eq:BayesFactor} then? This turns out to depend on the choice we make for $\Pr(\bn \mid \text{model})$, as follows.



\section{what model?}
\subsection{Categorical distributions} 
Consider the graphical model shown in Figure
\ref{fig:FreFriPGM_Categoricals}(a), which models both source and
background as Categorical distributions, with different parameters.

For $k$ categories (bins) we have two $k$-elt vectors $\bp$ of
probabilities.


Firstly, note that if the entire image counts $\bn = \bn^B + \bn^S$ were to be modelled by a {\it single} categorical distribution $\bp$,  the distribution of $\bn$ given $\bp$ would be multinomial:
\begin{align*}
\Pr(\bn \mid \bp) &= {\color{blue}{N \choose \bn}} \;\; \prod_k (p_k)^{n_k}
& \text{with $\displaystyle N= \sum_k n_k$ and $\sum_k p_k  = 1$}
\end{align*}
{\color{blue}where $\displaystyle {N \choose \bn} = {N \choose n_1,\ldots,n_K}$ is the "multinomial coefficient", $\displaystyle \frac{N!}{\prod_k (n_k!)}$.}

When the counts are {\bf split into the two classes} with different Categorical distributions, the probability is a
product of the multinomials for each class:
\begin{align}
  \Pr(\bn^B, \bn^S\mid N^B,N^S,\bp^B,\bp^S) &= 
  \Pr(\bn^B \mid N^B,\bp^B) \;\;\Pr(\bn^S \mid N^S,\bp^S) \nonumber \\
&= 
{\color{blue} 
{N^B \choose \bn^B}  \; {N^S \choose \bn^S}} \;\;\; \prod_k (p^B_k)^{n^B_k} \;\; \prod_{k^\prime} (p^S_{k^\prime})^{n^S_{k^\prime}}  
%\nonumber \\
\intertext{and note that if $\bp^B = \bp^S$ the products "merge", leaving}
&= 
{\color{blue}{N^B \choose \bn^B}  \; {N^S \choose \bn^S}} \;\;\; \prod_k (p_k)^{n_k} \label{eq:multinomial-split}
\end{align}

{\color{red} (....So the border does matter. I'm still not confident that I really understand this.)}

The probability of the {\bf actual (discretized) observations} is the same, but without the {\color{blue}blue} multinomial coefficients:
\begin{align}
  \Pr(\bz^B, \bz^S\mid N^B,N^S,\bp^B,\bp^S) &=  \prod_k (p^B_k)^{n^B_k} \;\; \prod_{k^\prime} (p^S_{k^\prime})^{n^S_{k^\prime}}  
\end{align}
which obviously factors beautifully and is easy to calculate.

\begin{figure}
  \begin{tabular}[c]{ccc}
\includegraphics[scale=1.0]{./pics/FreFriPGM_Categoricals} & \hspace{2cm} &
\includegraphics[scale=1.0]{./pics/FreFriPGM_DirMults} \\
(a)& & (b)
  \end{tabular}
\caption{A probabilistic graphical model (PGM) for the counts observed
  outside ($n^B$) and inside ($n^S$) a region of the image
  parameterised by $\theta$. $p$ is just some prior over $\theta$
  (eg. we have an inkling of what typical sizes might be, and limits
  on how elliptical, etc).  We {\it optimize} $\theta$ to find a
  specific region that is most strongly indicative of source ($\bp^S$)
  as opposed to background ($\bp^B$) statistics.
  \label{fig:FreFriPGM_Categoricals} 
}
\end{figure}


{\sc What's wrong with this as a model:}
\begin{itemize}
\item what distribution to use for $\bp^S$?
\item i really need to say what the nasty effect of assuming [the wrong / a single] $\bp^S$ is going to be. EXPLAIN!!
\end{itemize}

Open question: should the blue stuff be in, or not? Since $\theta$ affects the counts $\bn$ the blue terms will actually affect things!


\subsection{Dirichlet compound multinomial distributions instead} 
FreanFriedlanderJohnsonHollitt2013 ({\sc FreFri}) used the two-class model in
which $\balpha^0$ has large numbers in it and corresponds to the
background, and $\balpha^1 = (1,1,\ldots,1)$ representing our
ignorance regarding the source distribution. We used the ratio of the
associated posterior probs (a.k.a. the ``Bayes factor'') for the pixel
values in a region as a ``score'' for the sourciness of that
region. The source parameters $\theta$ could be thought of as
parameters specifying the border between source and background if you
like. We just have a strong prior we're building in that this border
tends to be elliptical.  Our procedure is: move region parameters
$\theta$ to increase Bayes Factor\footnote{Actually the ratio of
  \emph{posterior} probabilities rather than just the likelihoods,
  which leads to an additional additive constant.} $\log
\frac{\DM(\vec{n}^1 \given \balpha^1)}{\DM(\vec{n}^1 \given
  \balpha^0)}$.  Thus optimizing the score in the space of region
parameters amounts to source finding.






Consider a vector whose elements are categorical variables $y \in \mathcal{Y} $ where $\mathcal{Y} = \{Y_1,\ldots,Y_K \}$
\[
\by = (y_1,y_2, \ldots, y_N) \]
%We will use $\mathbb{Y}$ to denote the same as a set (ie. without the ordering imposed by the vector). 
The number of times each category occurs (its "count") forms a vector
$ \bn = n_1,\ldots,n_K  $. We will denote the {\it total} count $\sum_{k=1}^K n_k$ by $N$.

Now consider splitting the ``image'' $\by$ into two parts, e.g. are some pixel mid-way along its length:
\begin{align*}
\by =& (\by^A, \by^B)
\end{align*}
To connect this to radioblobs, think of $\by^A$ as the values inside a putative blob region and $\by^B$ as the rest (the "background").

\subsection{a 2-class example, for the intuition}

As a simple example consider a reasonably fair ``coin'' that
generates \texttt{0} or \texttt{1}. Let's call it's probability of
generating \texttt{1} its ``bentness'', $p_1$).  Consider the vector
of events being generated by repeatedly tossing this coin
$\mathtt{0000111111110000}$.  This {\it sequence} has exactly the same
probability of occuring as, say, the sequence
$\mathtt{1001101110010010}$, because they have the same overall counts
and the individual elements are generated i.i.d.

\section{Working with the Dirichlet compound Multinomial distribution instead of Multinomial}
A Dirichlet compound Multinomial (DM) distribution has parameters
$\balpha = \alpha_1,..,\alpha_K$ with $\alpha_k > 0$ and we denote $ A
= \sum_k^K \alpha_k$. Under the DM distribution, the probability of
generating the particular {\it set of individual outcomes}
$\by$ is
\begin{align}
\DM(\by \mid\balpha) &= \frac{\Gamma(A)}{\Gamma(N+A)} \prod_k \frac{\Gamma(n_k+\alpha_k)}{\Gamma(\alpha_k)}  \label{eq:DMvalues}
\end{align}

The gamma function just generalises the factorial to the reals: at
positive integer values they match, except for the slightly annoying
fact that the argument for gamma is larger by one: $\Gamma(z) =
(z-1)!$.

The probability of generating a particular vector of {\it counts}
({\it ie.} the counts of equivalent items in $\mathbb{Y}$) is almost
the same, but includes the multinomial coefficient out front to take
proper account of equivalent permutations:
\begin{align}
\DM(\bn \mid\alpha) &= {\color{blue} {N \choose \bn}} \;
 \frac{\Gamma(A)}{\Gamma(N+A)} \;\prod_k \frac{\Gamma(n_k+\alpha_k)}{\Gamma(\alpha_k)}  \label{eq:DMcounts}
\end{align}

\subsection{aside: a more succinct notation}

The multinomial coefficient can be rewritten in terms of gamma functions:
\[
\frac{N!}{\prod_k (n_k!)} \;
\;\;\; = \;\;\; 
\frac{\Gamma(N+1)}{\prod_k \Gamma(n_k+1)}
\]
which has the same form as the other terms now.
Perhaps we can simplify things then. Let's define a function
\begin{align}
  \label{eq:omega}
 \Omega(\bc) \;&= \;\;\frac{\Gamma(C)}{\prod_k \Gamma(c_k)} \;\;=\;\; \frac{(C-1)!}{\prod_k (c_k-1)! } & \text{where $C=\sum_k c_k$}
\end{align}

Now we can write the DM probability in equation \ref{eq:DMcounts}
using $\Omega$. Apart from anything else, this reduces clutter!
Again, terms that come from the multinomial coefficient are shown in
{\color{blue}blue}.
\begin{align}
\DM(\bn \mid\balpha) &= 
\frac{{\color{blue}\Omega(\bn+1)} \;\; \Omega(\balpha)}{ \Omega(\bn + \balpha)}
\label{eq:DMcounts_inOmega}
\end{align}


\subsection{what's the corollary of equation \ref{eq:multinomial-split}, with $\DM$ in place of the multinomial distribution?}

The probability of the overall counts in the whole sequence is:
\begin{align*}
\DM(\bn \mid N,\alpha) &=  \frac{{\color{blue}\Omega(\bn+1)} \;\; \Omega(\balpha)}{ \Omega(\bn + \balpha)}
\end{align*}

However the probability of the counts {\bf split into those two sections} must be:
\begin{align*}
  \Pr(\bn^A, \bn^B\mid N^A,N^B,\alpha) &=  
\frac{{\color{blue}\Omega(\bn^A+1)} \;\; \Omega(\balpha)}{ \Omega(\bn^A + \balpha)}
\;\;\; \frac{{\color{blue}\Omega(\bn^B+1)} \;\; \Omega(\balpha)}{ \Omega(\bn^B + \balpha)}
\end{align*}
and therefore...


Well, {\color{red} as before, the {\color{blue}blue} parts can't be
  reconciled - same as with the multinomial case. I'm still confused
  as to whether we SHOULD be including the blue terms!}

But also I'm {\color{OliveGreen}REALLY surprised that the non-blue terms don't ``collect''
like they do in the multinomial case. Why not?!}

\Line


\section{towards a generative model of astro images}

In our case, the counts $\bn$ are determined by the choice of region
parameterised by $\theta$, and so $\bn = \bn(\theta)$. For a single elliptical source, $\theta = (x,y,w_x,w_y,\phi)$ (ie. position,
widths, and rotation).

Denoting the derivative of the log gamma function
$\frac{\partial}{\partial n}\log \Gamma(n)$ by $\psi(n)$, the gradient
of $\log \DM$ w.r.t. $\theta$ is
\begin{align}
\frac{\partial}{\partial \theta} \log \DM(\bn(\theta) \mid\balpha) 
&= \sum_i W_i \, \frac{\partial n_i(\theta)}{\partial \theta} 
%\sum_i \bigg[  \psi(n_i+\alpha_i) - \psi(N+A) \bigg] \frac{\partial n_i}{\partial \theta}
\label{eq:gradientLogDM}
\intertext{where $W_i = \psi(n_i+\alpha_i) - \psi(N+A)$.}
\end{align}





\subsection{generative model}
Is our current scheme, in which we search for regions that have high
Bayes Factor values, equivalent or close to optimizing a parameterised
generative model of the \emph{image as a whole}? We haven't thought about it
in this way before, but writing down such a model would help us to
generalise the scheme to more than two classes.

A procedure for generating an image containing a \emph{single source}
`is as follows, assuming we're binning the pixel intensities into $I$ bins:
\begin{enumerate}
\item make up parameters $\theta$ describing a region of the image.
\item note that $\theta$ determines the numbers of pixels inside ($N^1$)  and  outside ($N^0$) the region.
\item set up $\balpha^1 \in \mathbb{R}^I$, with small numbers,
  and $\balpha^0 \in \mathbb{R}^I$, with big numbers, especially at the bins corresponding to low amplitudes.
\item the bin counts are DM distributed, but subject to constraints on $\sum_i n_i$ determined by the choice of $\theta$.
  \begin{itemize}
    \item    $\vec{n}^1 \sim \DM(\vec{n} \given \balpha^1)$
    \item $\vec{n}^0 \sim \DM(\vec{n} \given \balpha^0)$
  \end{itemize}
\end{enumerate}


\begin{figure}
\includegraphics[scale=1.0]{./pics/FreFriPGM_DirMults}
\caption{Here's what I think the {\sc FreFri} PGM looks like. Of the three latent variables, the two grey nodes are integrated out analytically through the magic of the DM distribution. And we {\it optimize} the orange one to find a specific region. 
The things I'd like to think about next are (i) how to represent an image with many many many sources, and (ii) how we might consider the $\alpha$ ``constants'' as learnable parameters instead.
\label{fig:FreFriPGM_DirMults}
}
\end{figure}

So this generative model of an image takes parameters $\theta,
\balpha^0, \balpha^1$, and results in just one ``data point''
consisting of the two vectors of counts $\vec{n}^0,\vec{n}^1$, which are sampled independently of one another.  The
likelihood and its logarithm are therefore
\begin{align*}
L =& \DM(\vec{n}^0 \mid \balpha^0) \; \cdot \; \DM(\vec{n}^1 \mid \balpha^1) 
\\ \\
\log L =& \log \DM(\vec{n}^0 \mid \balpha^0) \;\; + \;\; \log \DM(\vec{n}^1 \mid \balpha^1) 
%\\ \\ =& \log \Gamma(A^0) +\log \Gamma(A^1) - \log \Gamma(N^0+A^0)  - \log \Gamma(N^1+A^1) + \\ & \sum_i \log \Gamma(n^0_i + \alpha_i^0) - \log \Gamma(\alpha_i^0) + \sum_i \log \Gamma(n^1_i + \alpha_i^1) - \log \Gamma(\alpha_i^1) 
\end{align*}

This doesn't seem much like our Bayes factor though, since it
\emph{adds} instead of subtracts the two $\log \DM$ terms! The terms
are slightly different too.  Here they are, for direct comparison:

\begin{tabular}{|l|l|}
\hline
log DMR (Bayes Factor): & 
\parbox{.7\textwidth}{
\begin{align*}
&\log \DM(\vec{n}^1 \given \balpha^1) \;\;-\;\; \log \DM(\vec{n}^1 \given \balpha^0)
\end{align*}
} \\
\hline
Log L: & 
\parbox{.7\textwidth}{
\begin{align*}
& \log \DM(\vec{n}^1 \mid \balpha^1) \;\; + \;\; \log \DM(\vec{n}^0 \mid \balpha^0)
\end{align*}
} \\
\hline
\end{tabular}

Note that with $\log L$ it seems that we're modelling the whole image
in that $\bn^0$ is involved, but with the Bayes factor we are only
considering the current region. But in fact this is not so, and {\it
  the two are equivalent} up to an additive constant (so far as
$\theta$ is concerned).

Consider the log likelihood of the entire image under just the
``background'' model (or alternatively, consider the log likelihood in the case that we were to set $\balpha^1$ equal to $\balpha^0$):
\begin{align*}
\log \DM(\bn \mid \balpha^0) 
&= \log \DM(\bn^0(\theta) + \bn^1(\theta) \mid \balpha^0)  \\
&= \log \DM(\bn^0(\theta) \mid \balpha^0) \; + \; \log \DM(\bn^1(\theta) \mid \balpha^0) 
\end{align*}
This \emph{has to} be unaffected by our choice of region $\theta$,
ie. its derivative w.r.t. $\theta$ must be zero everywhere.  Adding
this constant to the original log likelihood immediately yeilds the
``Bayes factor'' score we've been using!

Put another way, the gradient w.r.t. any dimension of $\theta$ for the
log likelihood under the above generative model is the same as the
gradient of the ratio we used.  For the record, the gradient of our
Bayes factor (and thus the gradient of the log likelihood too) is...
\begin{align}
\frac{\partial}{\partial\theta}\text{DMR}(\theta) 
&= \sum_i \big[ \psi(n_i + \alpha^S_i) - \psi(n_i + \alpha^B_i) \big] \frac{\partial n^1_i}{\partial\theta} \notag\\
& - \;\; \big[\psi(N^1+A^1) - \psi(N^1+A^0)]\sum_i \frac{\partial n^1_i}{\partial\theta}
\end{align}


Describing things in terms of (the likelihood of) a generative model
for the image as a whole is going to be more useful in some contexts,
and especially for thinking about how to get beyond one source, and
two classes.



\section{Other stuff with Dirichlet}

Note the effect on $\Omega$ (equation \ref{eq:omega}) 
of incrementing one of the counts, say the one indexed $k^\star$:
\begin{align*}
\Omega(\bc) &\longrightarrow \Omega(\bc) \times \frac{C}{c_{k^\star}}
\intertext{whereas {\em de}crementing would change it to}
&\longrightarrow  \Omega(\bc) \times \frac{c_{k^\star}-1}{C-1}
\end{align*}

Incrementing the $k^{\star}$-th count changes the $\DM$ generative probability for the counts to:
\begin{align}
\DM(\bn \mid \balpha) &\longrightarrow  
\DM(\bn \mid \balpha)\;\; \times \;\;{\color{blue}\bigg( \frac{N+1}{n_{k^\star}+1} 
\bigg)}
\;\,\bigg(\frac{n_{k^\star}+\alpha_{k^\star}}{N+A}
\bigg)\label{eq:increment}
\intertext{whereas {\em de}crementing would change it to}
&\longrightarrow  
\DM(\bn \mid \balpha)\;\; \times \;\;{\color{blue}\bigg( \frac{n_{k^\star}}{N} 
\bigg)}
\;\,\bigg(\frac{N+A-1}{n_{k^\star}+\alpha_{k^\star}-1} 
\bigg)\label{eq:decrement}
\end{align}


\subsection{moving the boundary}
Consider the ``split'' image again, with the left-hand side denoted
$\by$ and the right by $\bz$.

The effect of ``moving the boundary'' to the right by one pixel is to
take a single value that was in $\bz$ and move it into $\by$
instead. For generality, I'm going to distinguish between the
$\balpha$ used in generating $\by$ and the one used for $\bz$ too.

Using equations \ref{eq:increment} and \ref{eq:decrement}, the log likelihood of $\by$ changes by
\begin{align*}
\log \DM(\bn_\Delta^y \mid \balpha^y) -\log \DM(\bn^y \mid \balpha^y) 
&=
{\color{blue}\log(N^y+1) - \log(n^y_{k^\star}+1)}
+ \log(n^y_{k^\star}+\alpha^y_{k^\star}) - \log(N^y+A^y)
\intertext{whereas that of $\bz$ changes by}
\log \DM(\bn_\Delta^z \mid \balpha^z) - \log \DM(\bn^z \mid \balpha^z) 
&=
{\color{blue}\log(n^z_{k^\star}) - \log(N^z)} + \log(N^z+A^z-1) - \log(n^z_{k^\star}+\alpha^z_{k^\star}-1)
\end{align*}

SO IF (big if...) $\DM(\by,\bz \mid \text{model}) = \DM(\by \mid
\balpha^y) \, \DM(\bz \mid \balpha^z) $, this means the overall log
likelihood changes by
\begin{align*}
\Delta \log \DM &= \log \DM(\bn_\Delta^y,\bn_\Delta^z) - \log \DM(\bn^y,\bn^z) \\
&= 
{\color{OliveGreen}  
\log \bigg(\frac{n^y_{k^\star}+\alpha^y_{k^\star}}{N^y+A^y} \bigg) 
}
\; - \;
{\color{blue}  
  \log \bigg(\; 
  \frac{n^y_{k^\star}+1}{N^y+1}
  \frac{N^z} {n^z_{k^\star}}
\bigg)
  } 
\; - \; 
\log \bigg(\frac{n^z_{k^\star}+\alpha^z_{k^\star}-1}{N^z+A^z-1}
\bigg)
\end{align*}
where the $n,N$ are the {\it old} counts.


\subsection{Bayes factor}
Alternatively...

Here's the Bayes factor comparing the likelihood of $\by$ under the two possible $\balpha$ priors:
\begin{align*}
\frac{\DM(\bn^y \mid \balpha^y)}{\DM(\bn^y \mid \balpha^z)} \;
 &= \;
\frac{{\color{blue}\Omega(\bn^y+1)} \;\; \Omega(\balpha^y)}{ \Omega(\bn^y + \balpha^y)} \;\;\;
\frac{ \Omega(\bn^y + \balpha^z)}{{\color{blue}\Omega(\bn^y+1)} \;\; \Omega(\balpha^z)} 
\intertext{Note that the effect of the multinomial coefficient {\it cancels out}, leaving}
&= \;\frac{\Omega(\balpha^y)}{\Omega(\balpha^z)} 
\;\;\;
\frac{ \Omega(\bn^y + \balpha^z)}{ \Omega(\bn^y + \balpha^y)} 
\end{align*}

We (if this is correct) have been calculating the log of this quantity.


So let's consider the effect of moving the boundary on the Bayes factor:
\begin{align*}
\frac{\DM(\bn_\Delta^y \mid \balpha^y)}{\DM(\bn_\Delta^y \mid \balpha^z)}
\; &= \; \frac{\Omega(\balpha^y)}{\Omega(\balpha^z)} 
\;\;\;
\frac{ 
 \Omega(\bn^y + \balpha^z) \;\times\; \frac{N^y+A^z}{n^y_{k^\star}+\alpha^z_{k^\star}}
}{
 \Omega(\bn^y + \balpha^y) \;\times\; \frac{N^y+A^y}{n^y_{k^\star}+\alpha^y_{k^\star}}
} 
\intertext{where $\bn_\Delta$ denotes the {\it new} counts. So...}
\text{new BF} \;&= \;\text{old BF} \; \times \; 
\bigg(
 \frac{N^y+A^z}{n^y_{k^\star}+\alpha^z_{k^\star}}
\bigg)
\; \bigg(\frac{n^y_{k^\star}+\alpha^y_{k^\star}}{N^y+A^y}
\bigg)
\intertext{so in log space, here's how the Bayes Factor changes when a single pixel of class $X_{k^\star}$ gets moved from $\bz$ into $\by$:}
\Delta \text{BF} \;&= 
{\color{OliveGreen}  
\log\bigg(\frac{n^y_{k^\star}+\alpha^y_{k^\star}}{N^y+A^y}
 \bigg)
}
\;\;-\;\;
\log\bigg( \frac{n^y_{k^\star}+\alpha^z_{k^\star}}{N^y+A^z}
\bigg) 
\end{align*}
where the $n,N$ are the {\it old} counts.

{\color{red} RED THOUGHT: how about we define a source region as simply some set of
connected pixels, and individually ask all the pixels at the
borderline (both out and in) whether they want to join / leave?
What would happen?...}


\end{document}
