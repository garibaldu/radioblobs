\documentclass{report}
%\usepackage{mathtools} 
\usepackage{amsmath}
\usepackage[parfill]{parskip}

\title{Gradient for 1D Dirichlet-multinomial score}
%\author{Anna Friedlander}


\begin{document}
\maketitle

This document contains some words and equations describing the multinomial Dirchlet score on 1D data and its gradient in 2D blob-space (window width $\times x$-position). The gradient will be used for gradient ascent to find maxima in blob-space (rather than exhaustively searching the whole space); and therefore the locations and sizes of potential sources: windows with a particular size and $x$-position that look suspicously ``different" from our background model.

\section{Introduction / Notation}


\subsubsection{representing an image in terms of binned values}

Astronomy images start from floats and it's common to ``bin'' these
under some predefined scheme. Within any given region in an image, we
can form a histogram of counts \textbf{n} in $K$ bins. We would like
to be able to give a score to the region that reflects how
``suspicious'' the histogram is in comparison to the histograms we
might expect were this to be a region of pure ``background'' signal.

Wherever possible we use $x$ to refer to a pixel in the image, and $k$
to refer to a bin index.  We denote the raw but binned image by
$\mathbf{b}$, and so $b_x$ is the index of the bin that results from
the pixel intensity at point $x$ in the image.

It will be useful to define the variable $C^x_k =\delta_{b_x,k}$ where
$\delta_{i,j}=1$ {\it iff} \, $i=j$, and is zero otherwise. Here $C$
is just an indicator variable - in effect, simply a verbose way of
representing the integer $b_x$ as an entire vector consisting of one 1
and many 0's. At this point it appears cumbersome, but we introduce it
in order to be able to extend it later to other normalised vectors,
thus allowing $C$ to represent ``partial counts'' across more than one
bin.  (Specifically in [[Dirichlet-bin-borders-section]] we introduce
a novel way to derive partial counts as a direct consequence of our
uncertainty about where bin borders should be placed).

We can then write the aggregated bin counts for some region $R$:
\begin{align}
n_k^{(\theta)} &= \sum_{x \in R} C^x_k
\end{align}

Such a region has hard (``all or nothing'') borders, whereas...


\subsubsection{defn of a soft region}
A region with ``soft'' borders can be defined by a weighting function $W^{(\theta)}_x$
over points $x$ in the region. The function's parameters $\theta$
could, for example, involve a center point $m$ and approximate half-width $\sigma$, as here:
\begin{align}
W^{(\theta)}_x 
&= \exp\left( -\frac{(x-m)^2}{2\sigma^2} \right) \label{eq:region-weight-in-1d} 
\end{align}

\subsubsection{bin counts from a region}
We can then write the aggregated bin counts for that region as
\begin{align}
n_k^{(\theta)} 
&= \sum_{x : b_{x}=k} W^{(\theta)}_x \\
&= \sum_{\text{all} \, x} C^x_k  \; W^{(\theta)}_x   \label{eq:soft-bin-counts} 
\end{align}

Note that in going to a region with ``soft'' borders we have had to
extend the sum to potentially all pixels in the image. However in
practice the sum over $x$ can be truncated to a local mask for which
$W^{(\theta)}_x \geq \epsilon$, for some suitably small value of
$\epsilon$.


\section{Scoring a histogram}



\subsection{The Dirichlet-multinomial distribution}

The Dirichlet-multinomial distribution is a compound probablility
distribution, where the parameter vectors $\textbf{p} = p_1 .. p_K$ of
a multinomial distribution is drawn from a Dirichlet distribution with
parameter vector $\boldsymbol\alpha = \alpha_1 .. \alpha_K$; the
probability that value $k$ is drawn from $p$ is given by $p_k$.

For some vector of counts \textbf{n} in $K$ bins of a histogram,
integrating out the multinomial distribution gives the following
marginal likelihood in terms of hyperparameters $\alpha$:

\begin{align}
P(\textbf{n}|\alpha) &= \frac{\Gamma(A)}{\Gamma(N+A)} \prod_k \frac{\Gamma(\textbf{n}_k+\alpha_k)}{\Gamma(\alpha_k)}  \label{eq:muldir} 
\intertext{where}
A &= \sum_k \alpha_k \\
N &= \sum_k \textbf{n}_k
\end{align}

The logarithm of this is: 
\begin{align}
\log P(\textbf{n}|\alpha) &= \log \Gamma(A) - \log \Gamma(N+A) + \sum_k \log \Gamma(\textbf{n}_k+\alpha_k) - \log \Gamma(\alpha_k) \label{eq:logmultdir}
\end{align}

\subsection{A sensible score}
A ``score" of how well a particular window conforms to our model of background can be calculated from the ratio of posterior probabilities under the two models (assuming an equal mixture of background and source in any particular sub-image) can be calculated as:

\begin{align}
\text{Score} &= \log \frac{P(\alpha^S | \textbf{n})}{P(\alpha^B | \textbf{n})} \label{eq:score} \\
&= \log \frac{P(\textbf{n} | \alpha^S)}{P(\textbf{n} | \alpha^B)} \;\; + \; \log \frac{P(\alpha^S)}{P(\alpha^B)}\label{eq:score2}
\end{align}

where $\alpha_S$ is the $\alpha$ vector for the Dirchlet distribution from which multinomial distributions across bins for source-regions are drawn; similarly $\alpha_ B$ is the $\alpha$ vector for the background Dirchlet distribution [describe how they are given values].

The second term in equation \ref{eq:score2} is a constant and so may be dropped, which leaves:

\begin{align}
\text{Score} &= \log \frac{P(\textbf{n} | \alpha^S)}{P(\textbf{n} | \alpha^B)} \\
&= \log {P(\textbf{n} | \alpha^S)} - \log {P(\textbf{n} | \alpha^B)} \label{eq:score3}
\end{align}

Writing out the score equation \ref{eq:score3} in full gives:

\begin{align*}
\log {P(\textbf{n} | \alpha^S)} - \log {P(\textbf{n} | \alpha^B)} &= \sum_k \log \Gamma (n_k + \alpha^S_k) - \log \Gamma (N + A^S) \\
&- \sum_k \log \Gamma (n_k + \alpha^B_k) + \log \Gamma (N + A^B) \label{eq:dirmult-score}
\end{align*}

The counts $\textbf{n}$ are those taken within a window whose parameters are $\theta$, so from now on we will write  $\textbf{n}^{(\theta)}$.


\section{The gradient of the score}

We can exhaustively explore the entire blob-space by iterating through all window sizes and all $x$-positions and calculating the score at each. However this is obviously time consuming and inefficient. A better approach is to perform gradient ascent to find maxima in blob-space.

We can take the derivative of the score (equation
\ref{eq:dirmult-score})with respect to window parameters $\theta$.
Denoting the derivative of the log of the $\Gamma$
function\footnote{available in the {\tt scipy} package as
  \texttt{scipy.special.basic.psi}.} by $\psi$, this gradient is:

\begin{align}
\frac{\partial}{\partial\theta}\text{Score}(\theta) 
&= \sum_k [\underbrace{\psi(n_k + \alpha^S_k) - \psi(n_k + \alpha^B_k)}_{\text{denote} \; Q_k}] \frac{\partial n_k^{(\theta)}}{\partial\theta}\\
& - \;\;\; [\psi(N+A^S) - \psi(N+A^B)]\sum_k \frac{\partial n_k^{(\theta)}}{\partial\theta}\\
&= \sum_k  Q_k \, \frac{\partial n_k^{(\theta)}}{\partial\theta}\;-\;
\sum_{i =1}^K
\underbrace{ \frac{1}{K}[\psi(N+A^S) - \psi(N+A^B)}_{\text{denote} \; Q_\text{base}}] \frac{\partial n_k^{(\theta)}}{\partial\theta} \\
&= \sum_{i=1}^K (Q_k - Q_\text{base}) \; \frac{\partial n_k^{(\theta)}}{\partial\theta}\\
\end{align}
with the above definitions for $Q_k$ and $Q_\text{base}$.

We can then calculate the remaining gradient term from equation
\ref{eq:soft-bin-counts}:
\begin{align}
\frac{\partial n_k^{(\theta)}}{\partial\theta} &= \sum_x \, C^x_k \; \frac{\partial W^{(\theta)}_x}{\partial\theta}
\end{align}


Pulling these together then, in general we have
\begin{align}
\frac{\partial}{\partial\theta}\text{Score}(\theta) 
&= \sum_k (Q_k - Q_\text{base}) \; \sum_x C^x_k \; \frac{\partial W^{(\theta)}_x}{\partial\theta} 
\label{eq:general-gradient}
\end{align}

Notice the ``data'' are involved in (up to) three places:
\begin{itemize}
\item $Q_k$, depends on $n_k$ the aggregated weighted counts.
\item$Q_\text{base}$, depends on the sum of $n_k$. Note that this does not vary with the position of the window (except for edge effects at the extent of the image), but does vary with the window's size.
\item $C^x_k$. Some simplification would be possible in the case that this is a strict ``indicator function'' (zero everywhere except one bin), but not in general.
\end{itemize}

%$C^x_k$ indexes the $x^{th}$ pixel/data-point, which falls into the $i^{th}$ bin.

\subsection{Gradient, given a specific parameterisation for $W$}
We are now free to choose a window function $W$, and plug it
into \label{eq:general-gradient} to arrive at the full score and its
gradient calculation.  Suppose we use the function for a 1-dimensional
window given in Equation \ref{eq:region-weight-in-1d}, whose
parameters $\theta$ are $(m, \sigma)$.

\subsubsection{$\partial \text{Score} / \partial m$}
The derivative with
respect to $m$ (the $x$-position of the mid-point of the window) is:
\begin{align}
\frac{\partial W^{(\theta)}_x}{\partial m} \; &= \; W^{(\theta)}_x \frac{(x-m)}{\sigma^2}
\intertext{and so}
\frac{\partial}{\partial m}\text{Score}(\theta) \;
&= \; \sum_k (Q_k - Q_\text{base}) \; \sum_x \, C^x_k  \; W^{(\theta)}_x \; \frac{(x-m)}{\sigma^2} \label{eq:1dgrad-wrt-position}
\end{align}


\subsubsection{$\partial \text{Score} / \partial \sigma$}
With respect to $\sigma$, we have

\begin{align}
\frac{\partial W^{(\theta)}_x}{\partial \sigma} &= W^{(\theta)}_x \frac{(x-m)^2}{\sigma^2} \frac{1}{\sigma}
\intertext{and so}
\frac{\partial}{\partial\sigma}\text{Score}(\theta) \;
&= \; \sum_k (Q_k - Q_\text{base}) \; \sum_x \, C^x_k  \; W^{(\theta)}_x \frac{(x-m)^2}{\sigma^3} 
\end{align}


\subsubsection{special case} 
In the simple case for which there is one set of bin borders, $C^x_k =\delta_{b_x,k}$ and so the gradient simplifies to
\begin{align}
\frac{\partial}{\partial m}\text{Score}(\theta) \;
&= \; \sum_x  (Q_{b_x} - Q_\text{base} ) \; W^{(\theta)}_x \; \frac{(x-m)}{\sigma^2}
\intertext{and}
\frac{\partial}{\partial \sigma}\text{Score}(\sigma) \;
&= \; \sum_x  (Q_{b_x} - Q_\text{base} ) \; W^{(\theta)}_x \; \frac{(x-m)^2}{\sigma^3}
\end{align}
Note the close correspondence between the two gradients, for this
parameterisation. 


\end{document} 
