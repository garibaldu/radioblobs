\documentclass{report}
%\usepackage{mathtools} 
\usepackage{amsmath}
\usepackage[parfill]{parskip}

\title{Gradient for 2D Dirichlet-multinomial score}
%\author{Anna Friedlander}


\begin{document}
\maketitle

This document contains some words and equations describing the multinomial Dirchlet score on 2D data and its gradient in five dimensional blob-space. The dimensions are: $x$-position, $y$-position, $\sigma_x$, $\sigma_y$, and rotation ($\phi$). The gradient will be used for gradient ascent to find maxima in blob-space (rather than exhaustively searching the whole space); and therefore the location, size, shape and rotation of potential sources: regions that look suspicously ``different" from our background model.

\section{Introduction / Notation}


\subsubsection{representing an image in terms of binned values}

Astronomy images start from floats and it's common to ``bin'' these
under some predefined scheme. Within any given region in an image, we
can form a histogram of counts \textbf{n} in $K$ bins. We would like
to be able to give a score to the region that reflects how
``suspicious'' the histogram is in comparison to the histograms we
might expect were this to be a region of pure ``background'' signal.

Wherever possible we use $(x,y)$ to refer to a pixel in the image, and $k$
to refer to a bin index.  We denote the raw but binned image by
$\mathbf{b}$, and so $b_{x,y}$ is the index of the bin that results from
the pixel intensity at point $(x,y)$ in the image.

It will be useful to define the variable $C^{x,y}_k =\delta_{b_{x,y},k}$ where
$\delta_{i,j}=1$ {\it iff} \, $i=j$ (that is, if pixel $i$ falls in bin $j$, and is zero otherwise. Here $C$
is just an indicator variable - in effect, simply a verbose way of
representing the integer $b_{x,y}$ as an entire vector consisting of one 1
and many 0's. At this point it appears cumbersome, but we introduce it
in order to be able to extend it later to other normalised vectors,
thus allowing $C$ to represent ``partial counts'' across more than one
bin.  (Specifically in [[Dirichlet-bin-borders-section]] we introduce
a novel way to derive partial counts as a direct consequence of our
uncertainty about where bin borders should be placed).

We can then write the aggregated bin counts for some region $R$:
\begin{align}
n_k^{\theta} &= \sum_{{x,y} \in R} C^{x,y}_k
\end{align}

Such a region has hard (``all or nothing'') borders, whereas...


\subsubsection{defn of a soft region}
A region with ``soft'' borders can be defined by a
weighting function $W^{\theta}_{x,y}$ over points $(x,y)$, with
parameters $\theta$ specifying the position and shape of the region.

For example, to specify an elliptical region, the function's
parameters $\theta$ could involve center coordinates $m_x$ and $m_y$,
and approximate half-widths in orthogonal direction $\sigma_x$ and
$\sigma_y$, and rotation parameter $\phi$. Without the rotation
parameter, the weighting function could be defined as here:
\begin{align}
W^{\theta}_{x,y}
&= e^f, \;\;
\text{   with   } \;\;
f = -\frac{\Delta_x^2}{2\sigma_x^2} - \frac{\Delta_y^2}{2\sigma_y^2} \label{eq:region-weight-in-2d-nophi}  
\intertext{where $\Delta$ signifies a displacement from the central position,}
\Delta_x &= x-m_x, \;\;\;\;\text{and} \;\;\; \Delta_y = y-m_y  \label{eq:Delta-defn}  
\end{align}

This elliptical region can be rotated by applying a rotation matrix to
$\Delta_x$ and $\Delta_y$:
\begin{align}
\begin{bmatrix}
\Delta_x^'\\
\Delta_y^'
\end{bmatrix}
=
\begin{bmatrix}
\cos(\phi) & -\sin(\phi)\\
\sin(\phi) & \cos(\phi)
\end{bmatrix}
\begin{bmatrix}
\Delta_x^\\
\Delta_y^
\end{bmatrix}
=
\begin{bmatrix}
\Delta_x \cos(\phi) - \Delta_y \sin(\phi)\\
\Delta_x \sin(\phi) + \Delta_y \cos(\phi)\\
\end{bmatrix} \label{eq:rotation-matrix}
\end{align}

The weighting function with the rotation matrix applied is therefore:
\begin{align}
W^{\theta}_{x,y} 
&= \exp\left(-\frac{\Delta_x^{'2}}{2\sigma_x^2} - \frac{\Delta_y^{'2}}{2\sigma_y^2}\right) \label{eq:region-weight-in-2d}
\end{align}
Expanding the terms $\Delta_x^{'2}$ and $\Delta_y^{'2}$ gives:
\begin{align}
\Delta_x^{'2}
&= (\Delta_x\cos(\phi)-\Delta_y\sin(\phi))^2 \\
&= \Delta_x^2 \cos^2(\phi) -2(\Delta_x\cos(\phi)\Delta_y\sin(\phi)) + \Delta_y^2\sin^2(\phi)\label{eq:delta_x_dash}
\end{align}
and:
\begin{align} 
\Delta_x^{'2}
&= (\Delta_x\sin(\phi)+\Delta_y\cos(\phi))^2 \\
&= \Delta_x^2 \sin^2(\phi) + 2(\Delta_x\sin(\phi)\Delta_y\cos(\phi)) + \Delta_y^2\cos^2(\phi)\label{eq:delta_y_dash}
\end{align}
Putting equations \ref{eq:delta_x_dash} and \ref{eq:delta_y_dash} back into \ref{eq:region-weight-in-2d} and simplifying yields:
\begin{align}
f &= - \;\left(a\Delta_x^2+2b\Delta_x\Delta_y+c\Delta_y^2 \right) \label{eq:region-weight-in-2d-simpl}
\intertext{with}
a &= \frac{\cos^2(\phi)}{2\sigma_x^2} + \frac{\sin^2(\phi)}{2\sigma_y^2} \label{eq:a-wgt} \\
b &= \frac{-\sin(2\phi)}{4\sigma_x^2} + \frac{\sin(2\phi)}{4\sigma_y^2} \label{eq:b-wgt} \\
c &= \frac{\sin^2(\phi)}{2\sigma_x^2} + \frac{\cos^2(\phi)}{2\sigma_y^2} \label{eq:c-wgt}
\end{align}



\subsubsection{bin counts from a region}

We can multiply the true counts by the windowing function to give ``weighted counts'', $\hat{C}$:
\begin{align}
\hat{C}^{x,y}_k &= C^{x,y}_k  \; W^{\theta}_{x,y}
\end{align}
(nb: to avoid clutter, $\hat{C}$'s dependence on $\theta$ is omitted in this notation).

We can then write the aggregated bin counts for the region defined by $\theta$ as
\begin{align}
n_k^{\theta} 
%  &= \sum_{(x,y) : b_{x,y}=k} W^{\theta}_{x,y} \\
% &= \sum_{\text{all} \, (x,y)} C^{x,y}_k  \; W^{\theta}_{x,y}   
&= \sum_{x,y} \; \hat{C}^{x,y}_k  \label{eq:soft-bin-counts} 
\end{align}


Note that in going to a region with ``soft'' borders we have had to
extend the sum to potentially all pixels in the image. However in
practice the sum over $(x,y)$ can be truncated to a local mask for which
$W^{\theta}_{x,y} \geq \epsilon$, for some suitably small value of
$\epsilon$.


\section{Scoring a histogram}



\subsection{The Dirichlet-multinomial distribution}

The Dirichlet-multinomial distribution is a compound probablility
distribution, where the parameter vectors $\textbf{p} = p_1 .. p_K$ of
a multinomial distribution is drawn from a Dirichlet distribution with
parameter vector $\boldsymbol\alpha = \alpha_1 .. \alpha_K$; the
probability that value $k$ is drawn from $p$ is given by $p_k$.

For some vector of counts \textbf{n} in $K$ bins of a histogram,
integrating out the multinomial distribution gives the following
marginal likelihood in terms of hyperparameters $\alpha$:

\begin{align}
P(\textbf{n}|\alpha) &= \frac{\Gamma(A)}{\Gamma(N+A)} \prod_k \frac{\Gamma(n_k+\alpha_k)}{\Gamma(\alpha_k)}  \label{eq:muldir} 
\intertext{where}
A &= \sum_k \alpha_k \\
N &= \sum_k n_k
\end{align}

The logarithm of this is: 
\begin{align}
\log P(\textbf{n}|\alpha) &= \log \Gamma(A) - \log \Gamma(N+A) + \sum_k \log \Gamma(n_k+\alpha_k) - \log \Gamma(\alpha_k) \label{eq:logmultdir}
\end{align}

\subsection{A sensible score}
A ``score" of how well a particular window conforms to our model of background can be calculated from the ratio of posterior probabilities under the two models (assuming an equal mixture of background and source in any particular sub-image) can be calculated as:

\begin{align}
\text{Score} &= \log \frac{P(\alpha^S | \textbf{n})}{P(\alpha^B | \textbf{n})} \label{eq:score} \\
&= \log \frac{P(\textbf{n} | \alpha^S)}{P(\textbf{n} | \alpha^B)} \;\; + \; \log \frac{P(\alpha^S)}{P(\alpha^B)}\label{eq:score2}
\end{align}

where $\alpha^S$ is the $\alpha$ vector for the Dirchlet distribution from which multinomial distributions across bins for source-regions are drawn; similarly $\alpha^B$ is the $\alpha$ vector for the background Dirchlet distribution [describe how they are given values].

The second term in equation \ref{eq:score2} is a constant and {\bf so may be dropped}\footnote{although it doesn't affect the gradient, i don't think it should be dropped: it sets the zero level for the score, which is important for interpretation}, which leaves:

\begin{align}
\text{Score} &= \log \frac{P(\textbf{n} | \alpha^S)}{P(\textbf{n} | \alpha^B)} \\
&= \log {P(\textbf{n} | \alpha^S)} - \log {P(\textbf{n} | \alpha^B)} \label{eq:score3}
\end{align}

Writing out the score equation \ref{eq:score3} in full gives:

\begin{align*}
\log {P(\textbf{n} | \alpha^S)} - \log {P(\textbf{n} | \alpha^B)} &= \sum_k \log \Gamma (n_k + \alpha^S_k) - \log \Gamma (N + A^S) \\
&- \sum_k \log \Gamma (n_k + \alpha^B_k) + \log \Gamma (N + A^B) \label{eq:dirmult-score}
\end{align*}

The counts $\textbf{n}$ are those taken within a window whose parameters are $\theta$, so from now on we will write  $\textbf{n}^{\theta}$.


\section{The gradient of the score}

We could, conceivably, explore the entire blob-space by exhaustively
iterating through all permutations of the parameter-vector $\theta$
($m_x$, $m_y$, $\sigma_x$, $\sigma_y$, $\phi$) over a range of values
of each parameter, and calculating the score at each. However this is
obviously time consuming and inefficient, and potentially intractable
with increasing sizes of images. A better approach is to perform
gradient ascent to find maxima in blob-space.

We can take the derivative of the score (equation
\ref{eq:dirmult-score})with respect to window parameters $\theta$.
Denoting the derivative of the log of the $\Gamma$
function\footnote{available in the {\tt scipy} package as
  \texttt{scipy.special.basic.psi}.} by $\psi$, this gradient is:

\begin{align}
\frac{\partial}{\partial\theta}\text{Score}(\theta) 
&= \sum_k [\underbrace{\psi(n_k + \alpha^S_k) - \psi(n_k + \alpha^B_k)}_{\text{denote} \; Q_k}] \frac{\partial n_k^{\theta}}{\partial\theta}\\
& - \;\;\; [\psi(N+A^S) - \psi(N+A^B)]\sum_k \frac{\partial n_k^{\theta}}{\partial\theta}\\
&= \sum_k  Q_k \, \frac{\partial n_k^{\theta}}{\partial\theta}\;-\;
\sum_{i =1}^K
\underbrace{ [\psi(N+A^S) - \psi(N+A^B)}_{\text{denote} \; Q_\text{base}}] \frac{\partial n_k^{\theta}}{\partial\theta} \\
&= \sum_{i=1}^K (Q_k - Q_\text{base}) \; \frac{\partial n_k^{\theta}}{\partial\theta}\\
\end{align}
with the above definitions for $Q_k$ and $Q_\text{base}$.

We can then calculate the remaining gradient term from equation
\ref{eq:soft-bin-counts}:
\begin{align}
\frac{\partial n_k^{\theta}}{\partial\theta} &= \sum_{x,y} \, C^{x,y}_k \; \frac{\partial W^{\theta}_{x,y}}{\partial\theta} \\
&= \sum_{x,y} \, C^{x,y}_k \;  W^{\theta}_{x,y} \; \frac{\partial f}{\partial\theta} \\
&= \sum_{x,y} \;\; \hat{C}^{x,y}_k \;\; \frac{\partial f}{\partial\theta}
\end{align}
where $f &= \; \log W^{\theta}_{x,y}$

Pulling these together then, in general we have
\begin{align}
\frac{\partial}{\partial\theta}\text{Score}(\theta) 
&= \sum_k (Q_k - Q_\text{base}) \; \sum_{x,y} \hat{C}^{x,y}_k \;\; \frac{\partial f}{\partial\theta}
\label{eq:general-gradient}
\end{align}

Notice the ``data'' enter in (up to) four places,
\begin{itemize}
\item $Q_k$, depends on $n_k$ the aggregated weighted counts.
\item$Q_\text{base}$, depends on the sum of $n_k$. Note that this does not vary with the position of the window (except for edge effects at the extent of the image), but does vary with the window's size.
\item $C^{x,y}_k$. Some simplification would be possible in the case that this is a strict ``indicator function'' (zero everywhere except one bin), but not in general.
\item $\frac{\partial f}{\partial\theta}$
\end{itemize}

{\it but always via the weighted counts $\hat{C}^{x,y}_k$. 
Presume the code should reflect this by pre-computing a $\hat{C}$ matrix
passing that around, instead of C and W.}
  
%$C^x_k$ indexes the $x^{th}$ pixel/data-point, which falls into the $i^{th}$ bin.


\subsection{Gradient, given a specific parameterisation for $W$}

We are now free to choose a window function $W$, and plug it
into \ref{eq:general-gradient} to arrive at the full score and its
gradient calculation.  Suppose we use the function for a 2-dimensional
ellipse given in Equation \ref{eq:region-weight-in-2d-simpl}, whose
parameters $\theta$ are $(m_x, m_y, \sigma_x, \sigma_y, \phi)$.

For example, the derivative with respect to $m_x$ is:
\begin{align}
\frac{\partial}{\partial m_x}\text{Score}(\theta) \;
&= \; \sum_k (Q_k - Q_\text{base}) \; \sum_{x,y} \, \hat{C}^{x,y}_k \;\; \frac{\partial f}{\partial m_x}
\intertext{and by differentiating equation \ref{eq:region-weight-in-2d-simpl} we have}
\frac{\partial f}{\partial m_x} \; &= \; 2a\Delta_x  + 2b\Delta_y 
\intertext{so the gradient calculation for this parameter is}
\frac{\partial}{\partial m_x}\text{Score}(\theta) \;
&= \; \sum_k (Q_k - Q_\text{base}) \; \sum_{x,y} \, \hat{C}^{x,y}_k \; \bigg[ \, 2a\Delta_x + 2b\Delta_y \bigg] \label{eq:2dgrad-wrt-delta-x}
\end{align}

More generally, differentiating equation
\ref{eq:region-weight-in-2d-simpl} with respect to each of the 5
parameters $m_x, m_y, \sigma_x, \sigma_y$ and $\phi$, we obtain the
following:
\begin{align*}
\frac{\partial f}{\partial m_x} \; &= \; 2a\Delta_x + 2b\Delta_x \\ \\ 
\frac{\partial f}{\partial m_y} \; &= \; 2b\Delta_x + 2c\Delta_y \\ \\ 
\frac{\partial f}{\partial \sigma_x} \; &= \;
\frac{1}{\sigma_x^3} \; \bigg( \Delta_x^2\cos^2(\phi) -
\Delta_x\Delta_y\sin(2\phi) + \Delta_y^2\sin^2(\phi) \bigg)\\ \\ 
\frac{\partial f}{\partial \sigma_y} \; &= \;
\frac{1}{\sigma_y^3} \; \bigg( \Delta_x^2\sin^2(\phi) +
\Delta_x\Delta_y\sin(2\phi) + \Delta_y^2\cos^2(\phi) \bigg) \\ \\ 
\frac{\partial f}{\partial \phi} \; &= \; \;
\frac{1}{2} \,\bigg( \frac{1}{\sigma_x^2} - \frac{1}{\sigma_y^2}
 \bigg) \;
\bigg(  
(\Delta_x^2 - \Delta_y^2) \, \sin(2\phi) \; + \; 2 \Delta_x \Delta_y \cos(2\phi)
 \bigg)
\end{align*}

{\bf Anna - should check that last one is correct / works: I took the expression you had and simplified. }

Substituting these 5 numbers into equation \ref{eq:general-gradient}
gives the full gradient, which can now be provided to any
gradient-based optimization routine.

\end{document} 
